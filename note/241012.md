[TOC]



# MGC-RM

## GAT

GAT（Graph Attention Networks，图注意力网络）是一种图神经网络（GNN）的变体，它通过引入自适应权重分配机制来改进传统的图卷积网络（GCN）。GAT 通过学习邻居节点之间的注意力系数来动态地聚合邻居的信息，从而更好地捕捉图中节点之间的关系（**某种节点表示，对自己的表示中有权重的考虑邻居节点**）。

### GAT 的核心思想

GAT 的核心思想在于通过注意力机制为每个节点的不同邻居分配不同的权重，这样可以更加灵活地处理不同节点的重要性。具体来说，GAT 的关键步骤包括：

1. **初始化**：每个节点都有自己的特征表示（通常是特征向量）。
2. **注意力机制**：通过计算注意力系数（attention coefficients）来**决定邻居节点的重要性**。
3. **聚合**：根据注意力系数加权**聚合**邻居节点的信息。
4. **更新**：使用聚合的信息来**更新每个节点的特征表示**。

### GAT 的数学描述

以下是 GAT 的数学描述：

#### 1. 初始化

假设有图 \( $G = (V, E)$ \)，其中 \( $V$ \) 是节点集，\( $E$ \) 是边集。每个节点 \($v_i$\) 有一个特征向量 \( $\mathbf{h}_i $\)。

#### 2. 注意力机制

定义注意力系数 \( $\alpha_{i,j}$ \)，它是节点 \($ v_i $\) 和 \($v_j$\) 之间的关系强度的度量。GAT 使用共享的注意力机制 \($\alpha$\) 来计算注意力系数：

[$e_{ij}=\text{LeakyReLU} (\mathbf{a}^T [\mathbf{Wh}_i || \mathbf{Wh}_j]) $]

 ==单层的全连接层，表示一种：一对向量到一个标量的映射，表示两个向量的相关度==

其中：

- \( $\mathbf{a}$ \) 是一个可学习的向量，用于计算注意力分数。
- \( $\mathbf{W}$ \) 是一个权重矩阵，用于变换节点特征。
- \( $||$ \) 表示特征向量的拼接（concatenation）。
- \( $e_{ij} $\) 是未归一化的注意力分数。

接下来，对每个节点 \( $v_i$ \) 的邻居节点计算注意力系数：

\[ $\alpha_{ij} $= $\frac{\exp(e_{ij})} {\sum_{k \in \mathcal{N}(i)} \exp(e_{ik})} $\]  ==针对一个节点，其邻居对其的权重系数 。所有邻居归一化处理，分配权重 softmax用于归一化==

其中 \( $\mathcal{N}(i)$ \) 是节点 \( $v_i$ \) 的邻居集。

#### 3. 聚合

使用注意力系数加权聚合邻居节点的信息：

\[ $\mathbf{h}_i^{'} = \sigma \left( \sum_{j \in \mathcal{N}(i)} \alpha_{ij} \mathbf{W} \mathbf{h}_j \right)$ \]

其中 \( $\sigma$ \) 是激活函数，如 $ReLU$。

#### 4. 更新

更新节点 \($ v_i $\) 的特征表示：

\[ $\mathbf{h}_i^{'} = \text{Concat}(\mathbf{h}_i^{(1)}, \mathbf{h}_i^{(2)}, \ldots, \mathbf{h}_i^{(K)}) $\]

这里 \( $K$ \) 是注意力头的数量，通常使用多头注意力机制来增强模型的表达能力。

> 内积表示向量的相似度

### 点积表示相关性

在 GAT（Graph Attention Networks）中，$LeakyReLU(\mathbf{a}^T [\mathbf{Wh}_i || \mathbf{Wh}_j])$ 用于计算两个节点 \( $v_i $\) 和 \( $v_j$ \) 之间的注意力系数。这个表达式实际上是一个自定义的函数，用于衡量两个节点之间的相似度或相关性。下面详细解释一下各个组成部分及其意义：

#### 解释各个组成部分

1. **\($\mathbf{Wh}_i$\) 和 \($\mathbf{Wh}_j$\)**：
   - \($\mathbf{W}$\) 是一个权重矩阵，用于将原始节点特征向量 \($\mathbf{h}_i$\) 和 \($\mathbf{h}_j$\) 转换成新的特征表示。
   - \($\mathbf{Wh}_i$\) 和 \($\mathbf{Wh}_j$\) 分别表示节点 \( $v_i$ \) 和 \( $v_j$ \) 在经过线性变换后的特征向量。

2. **\([$\mathbf{Wh}_i$ $||$ $\mathbf{Wh}_j$]\)**：
   - \(||\) 表示向量拼接（concatenation）操作，即将两个向量合并成一个新的向量。这样做是为了保留两个节点的特征信息，并通过拼接后的向量来计算它们之间的关系。
   - 例如，如果 \($\mathbf{Wh}_i$\) 和 \($\mathbf{Wh}_j$\) 都是 \(d\)-维向量，则 \([$\mathbf{Wh}_i $$||$ $\mathbf{Wh}_j$]\) 将是一个 \(2d\)-维向量。

3. **\($\mathbf{a}^T$\) 和 \($\mathbf{a}$\)**：
   - \($\mathbf{a}$\) 是一个可学习的向量，用于衡量两个节点特征向量的相似度或相关性。
   - \($\mathbf{a}^T$\) 表示 \($\mathbf{a}$\) 的转置，使用转置后可以进行点积运算。

4. **\($\mathbf{a}^T [\mathbf{Wh}_i || \mathbf{Wh}_j]$\)**：
   - 这个表达式计算了 \($\mathbf{a}$\) 向量与拼接后的特征向量 \($[\mathbf{Wh}_i || \mathbf{Wh}_j]$\) 的点积。
   - 点积的结果是一个标量，用于衡量两个节点特征向量的相似度或相关性。

5. **\($\text{LeakyReLU}(x)$\)**：
   - LeakyReLU 是一个激活函数，用于引入非线性。LeakyReLU 的定义如下：
     $\text{LeakyReLU}(x) = \begin{cases}
     x & \text{if } x > 0 \\
     \alpha x & \text{otherwise}
     \end{cases}
     $
   - 其中 \(\alpha\) 是一个小的正数（例如 0.01），用于防止梯度消失问题。

#### 为什么可以用来表示相关度

1. **衡量相似度**：
   - \($\mathbf{a}^T [\mathbf{Wh}_i || \mathbf{Wh}_j]$\) 计算的结果是一个标量，这个标量反映了节点 \($ v_i $\) 和 \($ v_j $\) 之间的相似度或相关性。
   - 如果两个节点的特征向量相似，则它们的拼接向量与 \(\mathbf{a}\) 向量的点积也会较大。
- 假设 \($\mathbf{Wh}_i$\) 和 \($\mathbf{Wh}_j$\) 相似，意味着它们在向量空间中的位置接近，即它们之间的角度较小。在这种情况下：
  
     1. **点积的大小**：如果 \($\mathbf{Wh}_i$\) 和 \($\mathbf{Wh}_j$\) 相似，它们在相同方向上的投影较长，因此 \($\mathbf{Wh}_i$\) 和 \($\mathbf{Wh}_j$\) 的拼接向量 \($[\mathbf{Wh}_i || \mathbf{Wh}_j]$\) 在相同方向上的投影也较长。因此，当与 \($\mathbf{a}$\) 向量进行点积运算时，结果较大。
     
     2. **向量 \($\mathbf{a}$\) 的作用**：向量 \($\mathbf{a}$\) 是一个可学习的向量，用于衡量两个节点特征向量的相似度。如果 \($\mathbf{a}$\) 在 \($[\mathbf{Wh}_i || \mathbf{Wh}_j]$\) 的某些维度上有较大的权重，则这些维度上的相似性会对最终的点积结果贡献更大。

2. **引入非线性**：
   - LeakyReLU 作为激活函数，可以进一步增强模型的表达能力，使得注意力系数更具区分性。
   - LeakyReLU 保留了负值部分的信息，使得模型在处理负数特征时也能保持一定的敏感度。

#### 公式的意义

综合起来，公式 \($\text{LeakyReLU}(\mathbf{a}^T [\mathbf{Wh}_i || \mathbf{Wh}_j])$\) 的意义在于通过一个自定义的函数来衡量两个节点之间的相似度或相关性，并通过非线性激活函数进一步增强这种衡量的效果。

理解为什么“如果两个节点的特征向量相似，则它们的拼接向量与 \(\mathbf{a}\) 向量的点积也会较大”可以从向量空间的角度来解释。这里的关键在于理解点积运算的性质以及特征向量相似性的含义。

在 GAT（Graph Attention Networks）中，激活函数在计算注意力系数（attention coefficients）的过程中起着重要的作用。具体来说，在 GAT 的注意力机制中，使用了 LeakyReLU 激活函数来处理节点特征向量的点积结果。下面我们详细探讨激活函数在注意力系数公式中的意义。

### 激活函数的作用

1. **引入非线性**：
   - 激活函数的主要作用之一是引入非线性，使得模型能够拟合更复杂的函数关系。如果没有激活函数，整个模型就相当于一个线性模型，其表达能力有限。
   - LeakyReLU 是一种常用的激活函数，它在正数部分保持线性，而在负数部分引入了一个小的斜率，避免了 ReLU 的“死区”问题。

2. **标准化输出**：
   - 激活函数可以对输出进行标准化，使其落在一个特定的范围内。在 GAT 中，LeakyReLU 可以确保注意力系数 \(e_{ij}\) 的值不会过大或过小，从而影响后续的归一化操作。
   - 通过 LeakyReLU，即使输入为负数，输出也不会完全为零，而是保留了一定的信息。

3. **增强区分性**：
   - 激活函数可以增强不同节点之间的区分性。LeakyReLU 可以放大或缩小输入信号的幅度，从而使得不同节点之间的注意力系数更有区分性。
   - 例如，对于两个相似的节点，它们的注意力系数可能会非常接近，但在经过 LeakyReLU 激活之后，它们的差异会被放大，从而更好地捕捉节点之间的关系。

4. **数值稳定性**：
   - 激活函数可以帮助提高数值稳定性。LeakyReLU 可以避免 ReLU 在负数区域的梯度消失问题，从而有助于训练过程中的梯度传播。

GAT（Graph Attention Networks）是一种基于图结构数据的深度学习模型。它主要用于处理图数据中的节点分类、边预测等问题。GAT的设计目的是为了在图中捕捉局部结构信息，并且赋予不同的邻居节点不同的权重，这通过注意力机制来实现。

### GAT的基本训练流程：

1. **数据准备**：
   - 首先需要准备好图数据，包括**节点特征矩阵**、**邻接矩阵**（表示节点之间的连接关系）等。
   - 对于监督学习任务，还需要准备标签数据。

2. **模型构建**：
   - 定义GAT层：每一层中，节点会根据其邻居的信息以及它们之间的关系进行信息传递，并通过注意力机制计算出不同邻居的重要性权重。
   - 可以堆叠多个GAT层来构建深层的网络结构。

3. **损失函数定义**：
   - 根据具体任务选择合适的损失函数，如交叉熵损失用于分类任务。

4. **优化器设置**：
   - 选择一个优化算法，如Adam或SGD，并设定学习率等超参数。

5. **训练过程**：
   - 使用前向传播计算输出。
   - 计算损失值，并使用反向传播更新模型参数。
   - 重复上述步骤直到达到预设的训练轮数或满足停止条件。

6. **评估与调整**：
   - 在验证集上评估模型性能，根据结果调整超参数或模型结构。
   - 使用测试集评估最终模型性能。

### 训练的模型架构：

GAT通常包含以下组件：

- 输入层：接收节点特征。
- 多个GAT层：每个GAT层都会为输入的节点特征计算一个注意力系数矩阵，然后利用这个矩阵对邻居节点的信息进行加权求和，得到新的节点特征表示。
- 输出层：根据任务需求，可能是分类层或其他类型的层。

在实际应用中，GAT可以和其他神经网络层结合使用，比如全连接层、池化层等，以适应更复杂的数据结构和任务需求。此外，为了防止过拟合，还可以在模型中加入正则化项或使用dropout技术。

## MSE

均方误差（Mean Squared Error, MSE）是一个非常常用的损失函数，特别是在回归任务中。它也可以用于其他类型的机器学习任务，比如自编码器中的重构误差、强化学习中的价值函数估计等。以下是MSE作为损失函数的一些原因：

1. **易于理解和计算**：

MSE公式简单直观，计算也非常直接。对于给定的一组预测值 \( \hat{y} \) 和真实值 \( y \)，MSE定义为：
\[ $\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$ \]
这里 \( $n$ \) 是样本数量，\( $y_i$ \) 是真实值，\( $\hat{y}_i$ \) 是预测值。

2. **对异常值敏感**：

由于MSE是预测误差的平方和，因此它会对较大的预测误差（即异常值）更加敏感。这意味着模型会被驱动去减小大的误差，从而可能提高总体的预测精度。

3. **导数简单**：

MSE的导数也很容易计算，这对于使用梯度下降法优化参数非常重要。MSE的导数为：
\[ $\frac{\partial \text{MSE}}{\partial \hat{y}} = 2(\hat{y} - y)$ \]
这意味着优化过程中计算梯度比较容易。

4. **平滑性**：

MSE是一个连续可微的损失函数，这使得它非常适合使用梯度下降法进行优化。相比之下，像绝对误差这样的损失函数在某些点上不可微，可能会导致优化过程中的问题。

总之，MSE作为一种损失函数具有许多优点，使得它在多种机器学习任务中都是一个有效的选择。然而，需要注意的是，MSE对于异常值的敏感性有时可能会成为缺点，特别是当数据集中存在很多异常值时。在这种情况下，可能需要考虑使用其他损失函数，如均值绝对误差（MAE）或Huber损失等。

## 相似度系数

![image-20241022215628930](C:\Users\lcr\AppData\Roaming\Typora\typora-user-images\image-20241022215628930.png)

您提供的公式看起来像是一个基于余弦相似度的加权版本，用于衡量两个向量 \( $h_m$ \) 和 \( $h_j$ \) 之间的相似度。在这个公式中，\( $N$ \) 表示一组参考向量的数量，\( $h_k$ \) 是这一组参考向量中的第 \( $k$ \) 个向量。让我们逐步解析这个公式：

1. **分子部分**：

   - \( $\cos(h_m, h_j)$ \) 是向量 \( $h_m $\) 和 \( $h_j$ \) 之间的余弦相似度。余弦相似度衡量了这两个向量的方向上的相似性，而不是大小。它的取值范围在 [-1, 1] 之间，其中 1 表示完全相同的方向，0 表示方向完全不同，-1 表示完全相反的方向。

     \($ \text{similarity} = \cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{|\mathbf{A}| |\mathbf{B}|} = \frac{\sum\limits_{i=1}^{n} A_i B_i}{\sqrt{\sum\limits_{i=1}^{n} A_i^2} \sqrt{\sum\limits_{i=1}^{n} B_i^2}}$ \)

2. **分母部分**：

   - \( $\sum_{k=1}^{N}\cos(h_m, h_k)$ \) 是向量 \( $h_m$ \) 与所有 \( $N$ \) 个参考向量的余弦相似度之和。这个和反映了 \( $h_m$ \) 与整个参考集的平均相似度。

3. **整体意义**：

   - 这个公式实际上是对 \( $h_m$ \) 和 \( $h_j$ \) 之间的余弦相似度进行了加权，权重由 \( $h_m$ \) 与其他所有参考向量的相似度决定。换句话说，**如果 \( $h_m$ \) 与大多数参考向量都高度相似，那么 \( $h_m$ \) 和 \($h_j$ \) 之间的相似度就会降低**，反之亦然。



# LOG

## 241101

长时间休眠和睡眠，期间打开主机进行打印，然后系统崩溃，修复系统时文件损坏，不能修复，于是重装了系统。

1. 写readme文件

## 241025

1. ==仅用一个图对训练出模型==
   
   1. 不需要训练的数据怎么快速通过模型获得结果
   
2. 他的模型没有考虑每个扰动图之间的关系
   
3. 图对能够把所有扰动图考虑进去吗？
   
4. 不同重要性节点的图对训练的模型有什么不同
   
2. loss有==尖刺==，（存在离群点干扰？），是否跟设备有关？

3. ==怎么调用已有的模型==，需要forward吗，好像不用

4. ==测试集==对比：
   1. 某扰动图模型的泛化能力，在其他扰动图上的损失效果： **代码变量调用可能有问题**
   
   2. ==大图==，改变节点后模型的能力： 模型中间的层需要更改??
   
      或者只改变节点个数，不改变特征的维数
      
      （节点个数会导致数据量变大，但似乎不影响层数。但是改变层数后模型行不收敛了）
      
      > 如果损失函数开始时几乎不变（或者说变化很小），然后逐渐开始收敛，这通常意味着以下几种情况之一：
      >
      > 1. **初始阶段数据稀疏或权重初始化不佳**：
      >    - 在训练初期，如果数据集很大，或者权重初始化不当（比如所有权重都初始化为零），那么模型可能需要一些时间来“热身”，即开始有效地调整其参数以适应数据。
      >
      > 2. **学习率设置**：
      >    - 如果使用的是一个固定的学习率（learning rate），且这个值设置得过小，那么在训练初期模型的更新可能非常缓慢，导致损失函数的变化不大。随着训练的进行，模型参数逐渐调整，损失函数开始下降并收敛。
      >
      > 3. **梯度消失或爆炸**：
      >    - 如果模型结构较深或者激活函数选择不当（如使用了sigmoid或tanh作为深层网络中的激活函数），可能会遇到梯度消失或梯度爆炸的问题。这会导致在训练初期权重更新非常缓慢甚至不动。解决这个问题的方法包括使用ReLU这样的激活函数，或者采用梯度裁剪等技术。
      >
      > 4. **局部最小值**：
      >    - 损失函数可能有多个局部最小值，在某些情况下，模型可能会陷入一个局部最小值，在这里损失函数的变化很小。随着进一步的训练，它可能会跳出这个局部最小值，并继续向全局最小值靠近。
      >
      > 5. **正则化效应**：
      >    - 如果使用了正则化技术（如L1或L2正则化），这些技术可以帮助避免过拟合，并可能导致损失函数在某些阶段变化较小。但是，随着训练的继续，模型会学习到更合适的参数，损失函数会继续下降。
      >
      > 6. **数据预处理**：
      >    - 数据预处理（如归一化、标准化）可以影响损失函数的变化速率。如果数据未正确预处理，可能会导致训练初期损失函数变化不大。
      >
      > 在任何情况下，重要的是要监控训练过程中的损失函数和其他相关指标，以确保模型能够有效地学习并且不会过拟合或欠拟合。如果有疑问，可以尝试调整学习率、使用不同的优化算法、改变模型架构或者增加/减少正则化强度等方法来改善训练过程。

==**计划**==

==先用效果好的小图导出模型，计算数值往后做==

## 241024

1. GAT 需要edge index ,但是edge index 不规则，不能npz

2. 构建graph pair 类

3. ```
   feature_1 = (feature_1 - mean_1) / std_1
   ```
   
   对每个特征进行标准化处理（也称为标准化或 z-score 标准化）是一种常用的数据预处理技术，目的是使每个特征具有零均值和单位方差。标准化处理可以改善数据的分布，从而提高机器学习模型的性能。以下是标准化处理的具体解释：
   
   #### **标准化处理公式**
   
   对于给定的一组数据 \($ \{x_1, x_2, \ldots, x_n\} $\)，其标准化处理公式为：
   
   \[$ z_i = \frac{x_i - \mu}{\sigma} $\]
   
   其中：
   - \( $z_i$ \) 是标准化后的数据；
   - \( $x_i$ \) 是原始数据；
   - \( $\mu$ \) 是原始数据的平均值（均值）；
   - \( $\sigma$ \) 是原始数据的标准差。
   
   #### **标准化的目的**
   
   1. **消除量纲影响**：不同特征可能有不同的量纲或尺度，标准化后可以消除这种差异，使特征在同一尺度上进行比较。
   2. **加速收敛**：在梯度下降等优化算法中，标准化后的数据可以使损失函数更快收敛。
   3. **提高模型性能**：对于一些机器学习算法（如支持向量机、线性回归等），标准化后的数据可以提高模型的性能。
   
   #### **总结**
   
   通过上述标准化处理，每个特征的平均值变为 0，标准差变为 1。这样可以使得特征在相同的尺度上进行比较，有助于提高模型的性能和收敛速度。如果你有进一步的问题或需要更具体的解释，请随时告诉我。
   
4. ```
   label_exp = torch.exp(-1 * label) # 无用
   ```

   您提到的将 `label` 转换为概率值的过程可以通过指数函数 \( $e^{-x}$ \) 来实现。这里我们来详细解释一下为什么这样做可以将正数转换为接近 0 的值，负数转换为接近 1 的值。

   #### **指数函数的性质**

   指数函数 \( $e^{-x}$ \) 有一些重要的性质：

   1. **当 \( x > 0 \)**：
      - \( $e^{-x}$ \) 的值会小于 1，并且随着 \( x \) 的增大，\( $e^{-x}$ \) 的值迅速趋近于 0。

   2. **当 \( x < 0 \)**：
      - \( $e^{-x}$ \) 的值会大于 1，并且随着 \( x \) 的减小，\( $e^{-x}$ \) 的值迅速趋近于无穷大。
      - 但是，如果我们关注的是 \( $e^{-|x|}$ \)，那么当 \( x < 0 \) 时，\( $e^{-x}$ \) 会变成 \( $e^{x}$ \)，此时 \($ e^{x}$ \) 的值会大于 1，并且随着 \( x \) 的减小，\( $e^{x}$ \) 的值会迅速趋近于 0。

   3. **当 \( x = 0 \)**：
      - \( $e^{-0} = e^{0} = 1$ \)。

   #### **举例说明**

   假设 `label` 是一个表示得分或距离的标量或向量，我们可以通过 \( e^{-x} \) 来将其转换为概率值：

   - **当 `label` 为正数**：
     - \( $e^{-\text{positive number}}$ \) 会接近 0，表示该得分或距离较高时，转换后的值接近 0。

   - **当 `label` 为负数**：
     - \( $e^{-\text{negative number}}$ \) 会接近 1，表示该得分或距离较低时，转换后的值接近 1。

   #### **修正为概率**

   如果我们希望将 `label` 转换为介于 0 和 1 之间的概率值，可以使用 Sigmoid 函数 \( $\sigma(x) = \frac{1}{1 + e^{-x}}$ \)。Sigmoid 函数确保所有的值都在 0 和 1 之间：

   #### **总结**

   通过 \( $e^{-x}$ \) 或 Sigmoid 函数 \( $\frac{1}{1 + e^{-x}}$ \)，可以将 `label` 转换为介于 0 和 1 之间的概率值。这种方法在很多场景中都非常有用，特别是在需要将得分或距离转换为概率值的情况下。如果你有具体的上下文或应用场景，请提供更多信息，以便我能给出更具体的解释和建议。

5. 粗心大意，输错矩阵，不是大小对不上GAT，是维度

6. ==如果用一个图对训练出模型，需要都训练一遍吗？？？==还是后面的其实不用

   1. 不需要训练的数据怎么快速通过模型获得结果
   2. 他的模型没有考虑每个扰动图之间的关系？？
   3. 还是说图对能够把所有扰动图考虑进去

7. ==测试集对比一下吧==

8. ==某个图对的loss有尖刺==，什么原理，是否跟设备有关？？

   
## 241023

1. 每个图对都分别计算相似度分数，计算N个？

与AI交流https://lxblog.com/qianwen/share?shareId=d1f6fc34-953b-4241-941d-a260f037fbaa

> 1. N个扰动图怎么凑图对：尽管存在N个扰动图，但每个扰动图都是独立地与基准图形成图对并通过相同的流程进行处理。
>
> 2. GAT中未扰动的节点也要学习表示？：GAT（图注意力网络）来学习节点表示时，即使某些节点本身没有改变，它们也会参与到其他节点特征的学习过程中，因为GAT是基于节点之间的相互作用来进行特征提取的。
>
>    此外，扰动图的设计目的是为了评估每个节点在网络中的贡献。通过对比扰动前后网络的变化情况，可以更好地理解每个节点的重要性。即使有些节点没有被直接扰动，它们在扰动图中的表现也会受到周围被扰动节点的影响，从而间接反映出它们在整个网络中的地位和功能。
>
>    综上所述，即使是那些在扰动图中没有直接发生改变的节点，它们的处理也不是多余的。
>
> 3. "图间"是？：相似度系数 $β_{mj}$ 和 $β_{in}$ 是用来衡量基准图 $G_c$ 中节点 $v_m$ 与扰动图 $G_o$ 中节点 $v^c_m$之间的相似度，以及基准图 $G_c$ 中节点 $v_i$与扰动图 $G_o$中节点 $v^i_c$之间的相似度。相似度系数是通过计算节点间表示向量的余弦相似度来获得的。
>
> 4. ”图间表示“的加权：图间表示是通过加权获得的原因在于它旨在捕捉两个图之间节点特征的关联性，并以此来反映节点在不同图中的相似性和重要性。这里所谓的“加权”，实际上是对节点在基准图和扰动图中表示的融合过程。
>
> 5. 加权的N：在相似度系数计算过程中，分母中的 N*N* 指的是图对中的另一个图中的节点数量。
>
> 6. 学习一个节点的图间表示，为什么$h^*_m$和$h^*_i$要计算2边：这种做法的目的在于评估节点在不同图结构下的表现一致性或差异性。
>
> 7. 全局读出：获得图的表示
>
>    基准图和扰动图的节点级图内（没有考虑彼此的相似度）和图间表示（考虑彼此的相似度）
>
> 8. 交叉匹配：图对相似，则图对的图内表示和图间表示之间的差异很小
>
>    图间表示：假如图对相似，他们的图内表示相似，求的余弦相似度是这个点和其它节点的，**如果笼统的看作是节点在图中的表现的话肯定是相似，但具体而言呢？**相似度系数肯定笼统而言，是相近的，加权后的表示也是。
>
> 9. 越重要，相似度越低
>

2. pycharm更新导致代码不能运行？？？

   退回版本后可以了

```
from . import _csparsetools
ImportError: DLL load failed while importing _csparsetools: 动态链接库(DLL)初始化例程失败。
```

3. 写成函数似乎运行速度会变慢？？（通过计时，发现不会）
4. MGC的数据读取部分，以及模型函数的调用部分需要重构

## 241022

最近一直在忙汇报和实验课

关于扰动图，因为要考虑每个节点的重要性，所以每个节点都有一个扰动图

![image-20241022205231377](C:\Users\lcr\AppData\Roaming\Typora\typora-user-images\image-20241022205231377.png)

扰动确实是只扰动一个节点

> GAT  

​		那我GAT只需要对这一个节点做？（**对每个节点都做，虽然某些节点没有改变，但是会参与到其他节点的学习中去，扰动图设计的目的是为了评估每个节点在网络中的贡献，虽然某些节点没有直接被扰动，它们在扰动图中的表现也会受到周围被扰动节点的影响，从而间接反映出他们在整个网络中的地位和功能**）

​		**本章均采用均方误差作为损失函数**

​		采用重构损失？常见的重构损失包括均方误差（MSE）或交叉熵损失

> 加权求和（相似度系数）

1. 用相似度系数加权，是在和谁加权，N是谁？？总节点？？邻居？

​		GAT中已经考虑了邻居，图间表示是哪一步抽象？

​		~~不同图之间加权，对N个扰动图进行加权？~~ 

​		~~每个节点的扰动特性来源于N个扰动图中分别计算出的每个节点的表示，~~

​		~~可是，这样的话，大部分节点的表示都是相同的，每个扰动图只有一个节点只差~~

​		原图和扰动图分别图间表示

2. 相似度系数，是谁和谁相似，该节点在每个扰动图之间的表示

(图间表示没有搞懂，从图级表示入手倒推理解一下)

## 241017

文件调用 

内存不够了，内存映射，但是好像没有效果

只扰动了一个点，某个点都一个扰动图

图对是每个节点都有一对吗

两个图分别GAT 学习图表示？$G_c$有很多个图，怎么学习图表示

用传感器测量值作为节点特征这合理吗？？一段时间的温度数据是这个点的气候变化特征，对这个特征进行抽象，有什么意义吗

数据的特征，是该数据的抽象，表征，类似label，但不是准确的label



## 241016

重新下载数据集

整理报告的论文

## 241015 

写实验报告

## 241014

看了看课程汇报的素材。 但是没有定下题目。

## 241013

完成了无线投屏，确实略有延迟，但是可用

## 241012

配置了SSH的基本连接

pycharm需要专业版才能ssh连接

无线显示器方案

