## 241122

1. > 使用排序的交叉熵作为损失函数来优化节点分数预测器是一种合理的方法，尤其是在你的目标不仅仅是预测准确的分数值，而是更关心预测分数与真实分数之间的相对顺序时。这种方法在推荐系统、信息检索等场景中非常常见，因为这些场景中通常更关注预测结果的排名顺序是否正确，而不是具体数值的精确度。
   >
   > ### 排序交叉熵损失函数
   >
   > 排序交叉熵（Ranking Cross Entropy Loss）通常用于衡量两个列表之间的差异，其中一个列表是模型预测的得分，另一个列表是真实的得分。这个损失函数鼓励模型预测的得分与真实得分之间的顺序尽可能一致。具体来说，如果模型预测的得分能够正确地反映真实得分的相对顺序，那么损失就会较低；反之，损失会较高。
   >
   > ### 实现方式
   >
   > 假设我们有 \( N \) 个节点，每个节点的真实分数为 \( $y_i$ \)，模型预测的分数为 \( $\hat{y}_i$ \)。我们可以定义排序交叉熵损失函数如下：
   >
   > 1. **计算每对节点的相对顺序**：
   >    - 对于每一对节点 \( (i, j) \)，如果 \( $y_i > y_j$ \)，则认为 \( i \) 应该排在 \( j \) 之前。
   >    - 记录这些相对顺序的标签 \( $t_{ij}$ \)，其中 \($ t_{ij} = 1 $\) 如果 \($ y_i > y_j $\)，否则 \($ t_{ij} = 0 $\)。
   >
   > 2. **计算预测得分的相对顺序概率**：
   >    - 使用一个激活函数（如 Sigmoid 函数）将预测得分转换为概率：
   >      $
   >      p_{ij} = \sigma(\hat{y}_i - \hat{y}_j) = \frac{1}{1 + e^{-(\hat{y}_i - \hat{y}_j)}}
   >      $
   >
   > 3. **计算排序交叉熵损失**：
   >    - 对于每一对节点 \( (i, j) \)，计算交叉熵损失：
   >      $
   >      L_{ij} = -t_{ij} \log(p_{ij}) - (1 - t_{ij}) \log(1 - p_{ij})
   >      $
   >    - 最终的总损失为所有对的平均损失：
   >      $
   >      L = \frac{1}{N(N-1)/2} \sum_{i < j} L_{ij}
   >      $
   >
   > ### 优点
   >
   > 1. **关注相对顺序**：这种损失函数直接优化了预测得分的相对顺序，使得模型更关注节点之间的相对位置，而不是具体的分数值。
   > 2. **鲁棒性**：对于一些应用场景，具体的分数值可能受到各种噪声的影响，而相对顺序则更为稳定和可靠。
   >
   > ### 缺点
   >
   > 1. **计算复杂度**：需要计算所有节点对的损失，计算量较大，特别是当节点数量较多时。
   > 2. **梯度稀疏**：由于损失函数只关注相对顺序，可能会导致梯度稀疏，优化过程可能会较慢。
   >
   > ### 总结
   >
   > 使用排序交叉熵作为损失函数来优化节点分数预测器是合理的，特别是在你更关心预测结果的相对顺序而非具体数值时。这种方法已经在许多实际应用中取得了良好的效果。不过，需要注意计算复杂度和梯度稀疏的问题，并根据具体情况进行调整和优化。

2. 损失值的大小会影响反向传播吗

   

## 241120

1. eval效果很差

2. 不对排序做训练，直接对预测值做训练呢

   分数是节点级的，只针对一个节点的预测？

   （分数，分数）（relu）（CrossEntropyLoss）（学习率调度器）

## 241119 cuda 梯度中断 CrossEntropyLoss 学习率调度器

1. cuda跑起来了，因为多个节点数据拼接过程中用的list导致梯度中断，所以之前没有梯度

   修正后打印参数梯度依然为none，但是loss有在下降，应该是没有选对参数

   但是cuda的训练并没有明显的提速，依然很慢

2. 直接用R_g训练的损失好像不正常，不如用R_g的排序训练

   ![_Training_Loss_epoch_200_lr_0.0001_20241119_151311cuda_scores](.\241012.assets\_Training_Loss_epoch_200_lr_0.0001_20241119_151311cuda_scores.svg)

   那是不是也应该用scores的排序训练？？

   梯度会消失

   ![_Training_Loss_epoch_200_lr_0.0001_20241119_152956cuda_R_g](.\241012.assets\_Training_Loss_epoch_200_lr_0.0001_20241119_152956cuda_R_g.svg)

   损失函数在训练过程中变得很大并且不再下降



1. > 当遇到损失函数不下降的情况，并且怀疑`loss.backward()`没有起作用时，可以考虑以下几个方面来排查和解决问题：
   >
   > #### 1. 损失函数定义
   > - **数值稳定性**：在你的代码中使用了`sigmoid`函数，这可能导致**梯度消失**问题。特别是在计算`log(F.sigmoid(y_hat_ij))`时，如果`y_hat_ij`非常大或非常小，`F.sigmoid(y_hat_ij)`可能接近于1或0，导致`log`的结果为负无穷或非常大的负数，从而影响梯度的计算。
   > - **除以组合数**：在返回损失值之前，你将总损失除以组合数`comb(len(scores), 2)`。这一步是合理的，但如果组合数非常大，可能会导致损失值变得非常小，进而影响梯度的大小。
   >
   > #### 2. 梯度检查
   > - **梯度是否为零**：可以通过打印`scores_tensor.grad`来检查梯度是否为零。如果梯度始终为零，那么`loss.backward()`确实没有产生有效的梯度更新。
   > - **模型参数梯度**：同样地，也可以检查模型参数的梯度，确保它们不是零或者非常小的值。
   >
   > #### 3. 优化器设置
   > - **学习率**：过高的学习率可能导致模型权重的更新过大，使得模型无法收敛；过低的学习率则可能导致模型训练非常缓慢，甚至看起来像是没有变化。尝试调整学习率，看看是否有改善。
   > - **优化器类型**：不同的优化器（如SGD、Adam等）对不同任务的表现可能有所不同。你可以尝试更换其他类型的优化器。
   >
   > #### 4. 模型架构
   > - **模型复杂度**：模型过于简单可能无法捕捉到数据中的复杂模式，而过于复杂的模型又可能导致过拟合。确保模型的复杂度适合你的任务。
   > - **初始化**：模型权重的初始化方式也会影响训练过程。确保你使用了一种合适的初始化方法。
   >
   > #### 5. 其他调试技巧
   > - **简化模型**：尝试用一个更简单的模型来测试同样的数据和损失函数，看是否能正常工作。这有助于确定问题是在数据处理、损失函数还是模型本身。
   > - **单步调试**：逐步执行代码，观察每一步的输出，特别是损失值的变化，以及梯度的计算。
   >
   > 如果你已经尝试了上述建议但仍然存在问题，可能需要进一步检查具体的实现细节，例如确保所有需要计算梯度的操作都在`autograd`机制下进行，确保张量的`requires_grad`属性被正确设置等。

2. > 这个警告信息表明你在访问一个**非叶子节点**（non-leaf Tensor）的 `.grad` 属性。在 PyTorch 中，只有叶子节点（leaf Tensor）才会默认保存梯度。如果你需要访问非叶子节点的梯度，可以使用 `retain_grad()` 方法。
   >
   > 具体来说，`scores_tensor` 是通过模型的前向传播计算得到的，因此它是一个非叶子节点。为了能够访问它的梯度，你需要在调用 `loss.backward()` 之前调用 `retain_grad()` 方法。
   >
   > 以下是修改后的代码示例：
   >
   > ```python
   >     scores_tensor = torch.Tensor(scores).requires_grad_(True).to(device)
   >     # 保留非叶子节点的梯度
   >     scores_tensor.retain_grad()
   > ```
   >
   > #### 解释
   > 1. **`scores_tensor.retain_grad()`**：这行代码确保 `scores_tensor` 的梯度会被保存，即使它不是一个叶子节点。
   > 2. **`loss.backward()`**：这行代码计算损失的梯度并将其存储在相应的张量的 `.grad` 属性中。
   >
   > #### 进一步调试
   > 如果问题仍然存在，可以尝试以下步骤：
   > - **打印梯度**：在 `loss.backward()` 之后，打印 `scores_tensor.grad` 和模型参数的梯度，确保它们不为零。
   > - **检查损失值**：确保损失值在每次迭代后有所变化，而不是一直保持不变。
   > - **简化模型**：尝试用一个更简单的模型来验证损失函数和优化器是否正常工作。
   >

3. > 如果 `scores_tensor.retain_grad()` 之后，`scores_tensor.grad` 仍然为 `None`，这通常意味着在反向传播过程中，`scores_tensor` 没有接收到任何梯度。以下是一些可能的原因和解决方法：
   >
   > #### 1. 确保 `scores_tensor` 是计算图的一部分
   > 确保 `scores_tensor` 是由模型的前向传播计算得到的，并且在计算图中是可导的。如果 `scores_tensor` 是从某个常量或独立的张量派生出来的，它可能不会参与到梯度计算中。
   >
   > #### 2. 检查损失函数的计算
   > 确保损失函数的计算没有断开计算图。例如，如果在损失函数中使用了某些操作，这些操作可能会导致梯度计算中断。常见的问题包括使用了 `.detach()` 或者 `.item()` 等方法。
   >
   > #### 3. 检查 `optimizer.zero_grad()`
   > 确保 `optimizer.zero_grad()` 被正确调用，否则之前的梯度可能会累积，导致新的梯度无法正确计算。
   >
   > #### 4. 打印中间变量
   > 在损失计算过程中，打印一些中间变量，确保它们的值和梯度都是正确的。
   >
   > #### 5. 确保 `requires_grad` 设置正确
   > 确保所有需要计算梯度的张量都设置了 `requires_grad=True`。
   >
   > #### 6. 使用 `torch.autograd.grad` 进行调试
   > 使用 `torch.autograd.grad` 可以手动计算梯度，帮助你定位问题。
   >
   > #### 示例代码
   > 以下是一个更详细的示例，包含了上述建议的调试步骤：
   >
   > ```python
   >     # 检查模型参数的梯度
   >     for name, param in ILGR_model.named_parameters():
   >         if param.grad is None:
   >             print(f"{name}.grad is None")
   >         else:
   >             print(f"{name}.grad:", param.grad)
   > ```
   >
   > #### 进一步调试
   > 如果上述步骤仍然不能解决问题，可以尝试以下方法：
   >
   > 1. **简化模型**：使用一个非常简单的模型（例如线性回归）来测试相同的损失函数和优化器，确保它们能正常工作。
   > 2. **手动计算梯度**：使用 `torch.autograd.grad` 手动计算梯度，确保梯度计算是正确的。
   >
   > ```python
   > # 手动计算梯度
   > gradients = torch.autograd.grad(loss, scores_tensor, retain_graph=True)
   > print("Manually computed gradients:", gradients)
   > ```
   >

4. > 理解梯度的概念对于调试深度学习模型非常重要。梯度表示损失函数对模型参数的敏感程度，即损失函数关于参数的偏导数。在反向传播过程中，梯度被计算出来并用于更新模型的参数。
   >
   > #### 可能导致梯度中断的操作
   >
   > 1. **`.detach()`**：将张量从计算图中分离出来，使其不再跟踪梯度。
   > 2. **`.item()`**：将张量转换为 Python 标量，通常用于获取单个元素的值。
   > 3. **`.numpy()`**：将张量转换为 NumPy 数组，不再跟踪梯度。
   > 4. **`.clone()`**：克隆张量，新张量默认不跟踪梯度。
   > 5. **条件语句**：在条件语句中使用张量的值，可能导致梯度计算中断。
   > 6. (**list 的拼接不对也有可能导致梯度中断，但是这里的梯度值应该是参与神经网络的变量的**)
   >
   > #### 如何打印其他中间变量的梯度
   >
   > 1. **确保中间变量是叶子节点或使用 `retain_grad()`**：
   >    ```python
   >    x = torch.tensor([1.0], requires_grad=True)
   >    y = x + 1
   >    y.retain_grad()  # 确保 y 保留梯度
   >    z = y * 2
   >    z.backward()
   >    print("x.grad:", x.grad)  # 输出 2.0
   >    print("y.grad:", y.grad)  # 输出 2.0
   >    ```
   >
   > 2. **使用 `torch.autograd.grad` 手动计算梯度**：
   >    ```python
   >    x = torch.tensor([1.0], requires_grad=True)
   >    y = x + 1
   >    z = y * 2
   >    gradients = torch.autograd.grad(z, [x, y], retain_graph=True)
   >    print("x.grad:", gradients[0])  # 输出 2.0
   >    print("y.grad:", gradients[1])  # 输出 2.0
   >    ```
   >
   > #### 变量的梯度到底是什么意思
   >
   > 梯度是一个向量，表示损失函数对模型参数的偏导数。在反向传播过程中，梯度被计算出来并用于更新模型的参数。具体来说：
   >
   > - **损失函数**：假设损失函数为 \( L \)，模型参数为 \( w \)。
   > - **梯度**：梯度 \( \nabla_w L \) 表示损失函数 \( L \) 对参数 \( w \) 的偏导数。
   >
   > 在 PyTorch 中，每个张量（如果 `requires_grad=True`）都有一个 `.grad` 属性，用于存储该张量的梯度。
   >
   > #### 总结
   >
   > 1. **确保所有需要计算梯度的张量都设置了 `requires_grad=True`**。
   > 2. **使用 `retain_grad()` 保留非叶子节点的梯度**。
   > 3. **检查损失函数的计算，确保没有断开计算图的操作**。
   > 4. **使用 `torch.autograd.grad` 手动计算梯度，帮助调试**。
   >

5. > 好的，让我们详细解释一下张量、梯度以及它们在深度学习中的作用。
   >
   > #### 张量（Tensor）
   >
   > 张量是一种多维数组，可以是一维（向量）、二维（矩阵）或多维。在 PyTorch 中，张量是基本的数据结构，用于表示数据和模型参数。张量可以包含任意维度的数据，不仅仅是标量。
   >
   > #### 梯度（Gradient）
   >
   > 梯度是一个向量，表示损失函数对模型参数的偏导数。在深度学习中，梯度用于指导模型参数的更新，以便最小化损失函数。具体来说：
   >
   > - **损失函数**：假设损失函数为 \( L \)，模型参数为 \( w \)。
   > - **梯度**：梯度 \( \nabla_w L \) 表示损失函数 \( L \) 对参数 \( w \) 的偏导数。
   >
   > #### 梯度的计算和应用
   >
   > 在 PyTorch 中，梯度的计算和应用主要通过以下步骤完成：
   >
   > 1. **前向传播**：计算模型的输出。
   > 2. **计算损失**：根据模型的输出和真实标签计算损失函数。
   > 3. **反向传播**：通过调用 `loss.backward()` 计算损失函数对所有==需要梯度的张量的梯度==。
   > 4. **更新参数**：使用优化器（如 SGD、Adam 等）更新模型参数。
   >
   > #### 详细解释
   >
   > 1. **前向传播**：
   >    - `scores[i] = ILGR_model(fea_list_tensor, R_g_tensor)`：通过模型前向传播计算模型的输出。
   >
   > 2. **计算损失**：
   >    - `loss = ranking_loss(scores_tensor, criticality_scores)`：根据模型的输出和真实标签计算损失函数。
   >
   > 3. **反向传播**：
   >    - `loss.backward()`：计算损失函数对所有需要梯度的张量的梯度。这一步会自动计算并存储梯度。
   >
   > 4. **更新参数**：
   >    - `optimizer.step()`：使用优化器更新模型参数。
   >
   > 5. **打印梯度**：
   >    - `if scores_tensor.grad is None:`：检查 `scores_tensor` 的梯度是否为 `None`。
   >    - `print("scores_tensor.grad:", scores_tensor.grad)`：打印 `scores_tensor` 的梯度。
   >    - `for name, param in ILGR_model.named_parameters():`：遍历模型的所有参数，检查并打印它们的梯度。
   >
   > #### 调试建议
   >
   > 1. **确保 `requires_grad=True`**：
   >    - 确保所有需要计算梯度的张量都设置了 `requires_grad=True`。
   >
   > 2. **检查梯度是否为 `None`**：
   >    - 如果梯度为 `None`，检查是否有断开计算图的操作（如 `.detach()`、`.item()` 等）。
   >
   > 3. **手动计算梯度**：
   >    - 使用 `torch.autograd.grad` 手动计算梯度，帮助调试。
   >
   > ```python
   > gradients = torch.autograd.grad(loss, [scores_tensor], retain_graph=True)
   > print("Manually computed gradients:", gradients)
   > ```
   >

6. > 在 PyTorch 中，"需要梯度的张量"是指那些在计算图中需要计算其梯度的张量。这些张量通常用于模型的参数更新。要使一个张量在计算图中需要计算梯度，需要将 `requires_grad=True` 设置为其属性之一。这样，当进行反向传播时，PyTorch 会自动计算并存储这些张量的梯度。
   >
   > #### 需要梯度的张量
   >
   > 1. **模型参数**：模型的权重和偏置通常需要计算梯度，因为这些参数会在训练过程中被更新。
   > 2. **中间变量**：在某些情况下，你可能需要计算中间变量的梯度，以便进行调试或进一步的计算。
   >
   > #### 示例
   >
   > 模型参数
   >
   > ```python
   > import torch
   > import torch.nn as nn
   > import torch.optim as optim
   > 
   > # 定义一个简单的线性模型
   > class SimpleModel(nn.Module):
   >     def __init__(self):
   >         super(SimpleModel, self).__init__()
   >         self.linear = nn.Linear(1, 1)
   > 
   >     def forward(self, x):
   >         return self.linear(x)
   > 
   > # 创建模型实例
   > model = SimpleModel()
   > 
   > # 创建优化器
   > optimizer = optim.SGD(model.parameters(), lr=0.01)
   > 
   > # 输入数据
   > x = torch.tensor([[1.0]], requires_grad=True)
   > y_true = torch.tensor([[2.0]])
   > 
   > # 前向传播
   > y_pred = model(x)
   > 
   > # 计算损失
   > loss = (y_pred - y_true) ** 2
   > 
   > # 反向传播
   > loss.backward()
   > 
   > # 打印模型参数的梯度
   > for name, param in model.named_parameters():
   >     print(f"{name}.grad:", param.grad)
   > ```
   >
   > 在这个例子中，`model.linear.weight` 和 `model.linear.bias` 都是需要梯度的张量，因为它们是模型的参数，会在训练过程中被更新。
   >
   > #### 中间变量
   >
   > ```python
   > import torch
   > import torch.nn.functional as F
   > 
   > # 创建一个输入张量
   > x = torch.tensor([1.0], requires_grad=True)
   > 
   > # 前向传播
   > y = x + 1
   > y.retain_grad()  # 保留 y 的梯度
   > z = y * 2
   > 
   > # 计算损失
   > loss = z ** 2
   > 
   > # 反向传播
   > loss.backward()
   > 
   > # 打印 x 和 y 的梯度
   > print("x.grad:", x.grad)
   > print("y.grad:", y.grad)
   > ```
   >
   > 在这个例子中，`x` 是一个需要梯度的张量，因为它需要计算梯度。`y` 也是一个需要梯度的张量，但默认情况下，非叶子节点不会保存梯度，所以我们需要调用 `y.retain_grad()` 来保留 `y` 的梯度。
   >
   > #### 梯度的作用
   >
   > 1. **参数更新**：在优化过程中，梯度用于更新模型的参数。例如，使用梯度下降法更新参数：
   >    
   >    $w_{\text{new}} = w_{\text{old}} - \eta \cdot \nabla_w L
   >    $
   >    其中，\( $\eta$ \) 是学习率，\($ \nabla_w L $\) 是损失函数对参数 \( w \) 的梯度。
   >
   > 2. **调试**：通过打印中间变量的梯度，可以帮助你调试模型，确保梯度计算正确。
   >
   > #### 总结
   >
   > - **需要梯度的张量**：这些张量在计算图中需要计算其梯度，通常包括模型参数和某些中间变量。
   > - **设置 `requires_grad=True`**：确保这些张量在计算图中需要计算梯度。
   > - **使用 `retain_grad()`**：对于非叶子节点，如果需要保留其梯度，需要调用 `retain_grad()`。
   >
   > 希望这些解释和示例能帮助你更好地理解“需要梯度的张量”及其在深度学习中的作用。如果有更多问题，请随时提问。

7. > ### 损失函数在训练过程中变得很大并且不再下降
   >
   > 当损失函数在训练过程中变得很大并且不再下降时，这可能是由多种原因引起的。以下是一些常见的问题及其解决方法：
   >
   > #### 1. **学习率过高**
   > - **问题**：学习率过高会导致梯度更新过大，使得模型权重在参数空间中跳跃，无法收敛。
   > - **解决方法**：降低学习率。可以尝试将学习率减半或减小一个数量级，观察损失函数的变化。
   >
   > #### 2. **梯度爆炸或梯度消失**
   > - **问题**：梯度爆炸（梯度过大）或梯度消失（梯度过小）都会导致模型无法有效学习。
   > - **解决方法**：
   >   - **梯度裁剪**：使用梯度裁剪（gradient clipping）来防止梯度爆炸。
   >   - **归一化**：使用批量归一化（Batch Normalization）或层归一化（Layer Normalization）来稳定梯度。
   >   - **激活函数**：使用 ReLU 以外的激活函数，如 LeakyReLU 或 SELU，这些激活函数可以缓解梯度消失问题。
   >
   > #### 3. **模型过拟合或欠拟合**
   > - **问题**：模型可能过拟合或欠拟合数据。
   > - **解决方法**：
   >   - **正则化**：增加 L1 或 L2 正则化项，防止模型过拟合。
   >   - **增加模型复杂度**：如果模型欠拟合，可以尝试增加模型的复杂度，如增加层数或神经元数量。
   >   - **数据增强**：增加训练数据的多样性，使用数据增强技术。
   >
   > #### 4. **损失函数不合适**
   > - **问题**：使用的损失函数可能不适合当前任务。
   > - **解决方法**：尝试使用其他损失函数，如均方误差（MSE）、交叉熵（Cross Entropy）等。
   >
   > #### 5. **数据问题**
   > - **问题**：数据可能存在问题，如数据不均衡、噪声过多等。
   > - **解决方法**：
   >   - **数据预处理**：检查数据预处理步骤，确保数据清洗和标准化正确。
   >   - **数据平衡**：如果数据不均衡，可以使用过采样或欠采样技术。
   >
   > #### 6. **优化器问题**
   > - **问题**：使用的优化器可能不适合当前任务。
   > - **解决方法**：尝试更换其他优化器，如 Adam、RMSprop 等。
   >
   > #### 7. **初始化问题**
   > - **问题**：模型参数的初始化可能不合适。
   > - **解决方法**：尝试使用不同的初始化方法，如 Xavier 初始化或 Kaiming 初始化。
   >
   > #### 8. **模型结构问题**
   > - **问题**：模型结构可能不适合当前任务。
   > - **解决方法**：重新设计模型结构，参考相关领域的最佳实践。
   >
   > #### 示例代码
   >
   > 以下是一个示例，展示了如何调整学习率和使用梯度裁剪：
   >
   > ```python
   > import torch
   > import torch.nn as nn
   > import torch.optim as optim
   > import torch.nn.functional as F
   > from scipy.special import comb
   > 
   > # 假设其他部分的代码已经定义好
   > # ...
   > 
   > # 定义损失函数
   > def ranking_loss(scores, true_ranks):
   >     loss = 0
   >     for i in range(len(scores)):
   >         for j in range(i + 1, len(scores)):
   >             r_ij = true_ranks[i] - true_ranks[j]
   >             y_hat_ij = scores[i] - scores[j]
   >             f_r_ij = F.sigmoid(r_ij.float())
   >             loss += -f_r_ij * torch.log(F.sigmoid(y_hat_ij)) - (1 - f_r_ij) * torch.log(1 - F.sigmoid(y_hat_ij))
   >     return loss / int(comb(len(scores), 2))
   > 
   > # 假设模型和数据已经定义好
   > ILGR_model = ...  # 你的模型
   > optimizer = optim.Adam(ILGR_model.parameters(), lr=0.001)  # 尝试降低学习率
   > un_location_list = ...  # 你的位置列表
   > un_fea_list = ...  # 你的特征列表
   > R_g = ...  # 你的 R_g 列表
   > criticality_scores = ...  # 你的真实排名
   > args = ...  # 你的训练参数
   > select_node = ...  # 选择的节点
   > location_list = ...  # 位置列表
   > fea_list = ...  # 特征列表
   > fea_list_tensor = ...  # 特征列表的张量
   > scores = []  # 存储模型输出的列表
   > loss_history = []  # 存储损失的历史记录
   > 
   > for epoch in range(args.max_epoch):  # 假设训练100个epoch
   >     for i, (location, fea) in enumerate(zip(un_location_list, un_fea_list)):
   >         location_list[select_node] = location
   >         fea_list[select_node] = fea
   >         ILGR_model.train()
   >         R_g_tensor = R_g[i]
   >         optimizer.zero_grad()
   >         scores[i] = ILGR_model(fea_list_tensor, R_g_tensor)
   >     
   >     scores_tensor = torch.stack(scores).requires_grad_(True).to(device)
   >     # 保留非叶子节点的梯度
   >     scores_tensor.retain_grad()
   >     loss = ranking_loss(scores_tensor, criticality_scores)
   >     
   >     # 打印损失值
   >     print('epoch:{}, loss:{}'.format(epoch, loss.item()))
   >     
   >     # 反向传播
   >     loss.backward()
   >     
   >     # 梯度裁剪
   >     torch.nn.utils.clip_grad_norm_(ILGR_model.parameters(), max_norm=1.0)
   >     
   >     # 更新模型参数
   >     optimizer.step()
   >     loss_history.append(loss.item())
   > ```
   >
   > #### 进一步调试
   >
   > 1. **可视化损失曲线**：绘制损失函数随训练轮次的变化曲线，观察是否存在震荡或饱和现象。
   > 2. **检查梯度**：在反向传播后，检查模型参数的梯度，确保它们不是零或非常小的值。
   > 3. **简化模型**：尝试用一个更简单的模型来测试相同的损失函数和优化器，确保它们能正常工作。
   >
   > 希望这些方法能帮助你解决损失函数不下降的问题！如果有更多问题，请随时提问。
   >
   > 

8. 修改RegressionModule的激活函数relu为Softmax

   epoch:0, loss:0.6791266798973083
   epoch:1, loss:0.6791266798973083
   epoch:2, loss:0.6791263818740845
   epoch:3, loss:0.6791262030601501
   epoch:4, loss:0.679125964641571
   epoch:5, loss:0.679125964641571

   损失下降很慢

9. ```
   scores_tensor_scores = torch.tensor(np.argsort(np.array(tensors))).to(device)
   ```

   对预测的分数进行排序，用排序进行学习

   **排序值过大过小很正常，但是变成概率后会有0值**
   
   r 30    y 35
   f_y 1.0   
   f_r 1.0    1-f_r 0.0
   log 0.0    1- log -inf
   loss nan
   
   **y过大**，导致F.sigmoid(y_hat_ij) = 1，log = 0, log0=-∞
   
   **r过大**，导致F.sigmoid(r_ij) = 1，则全看log的值
   
   > Sigmoid 函数是一种常用的激活函数，广泛应用于神经网络中，特别是在二分类问题中。Sigmoid 函数将实数域映射到 (0, 1) 区间内，这使得它特别适合用于将模型的输出转换为概率值。
   >
   > ### Sigmoid 函数的数学表达式
   >
   > Sigmoid 函数的数学表达式为：
   >
   > \[ \sigma(x) = \frac{1}{1 + e^{-x}} \]
   >
   > ### Sigmoid 函数的性质
   >
   > 1. **范围**：Sigmoid 函数的输出范围是 (0, 1)，这使得它非常适合用于表示概率。
   > 2. **单调性**：Sigmoid 函数是单调递增的，这意味着随着输入的增加，输出也会增加。
   > 3. **平滑性**：Sigmoid 函数是连续且可微的，这使得它可以用于梯度下降等优化算法。
   > 4. **对称性**：Sigmoid 函数在 x = 0 处是对称的，即 \(\sigma(0) = 0.5\)。
   >
   > ### Sigmoid 函数的图形
   >
   > Sigmoid 函数的图形是一个 S 形曲线，如下所示：
   >
   > ![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png)
   >
   > ### 在 PyTorch 中使用 Sigmoid 函数
   >
   > 在 PyTorch 中，可以使用 `torch.sigmoid` 函数来计算 Sigmoid 函数的值。以下是一个示例代码：
   >
   > ```python
   > import torch
   > import torch.nn.functional as F
   > 
   > # 创建一个示例张量
   > x = torch.tensor([-1.0, 0.0, 1.0, 2.0])
   > 
   > # 计算 Sigmoid 函数的值
   > sigmoid_values = torch.sigmoid(x)
   > 
   > # 或者使用 F.sigmoid
   > sigmoid_values = F.sigmoid(x)
   > 
   > # 打印结果
   > print("Input Tensor:")
   > print(x)
   > print("Sigmoid Values:")
   > print(sigmoid_values)
   > ```
   >
   > ### 输出
   >
   > ```
   > Input Tensor:
   > tensor([-1.0000,  0.0000,  1.0000,  2.0000])
   > Sigmoid Values:
   > tensor([0.2689, 0.5000, 0.7311, 0.8808])
   > ```
   >
   > ### Sigmoid 函数的应用
   >
   > 1. **二分类问题**：Sigmoid 函数常用于二分类问题，将**模型的输出转换为概率值**。例如，在 logistic 回归中，Sigmoid 函数用于将线性组合的输出转换为概率。
   > 2. **神经网络**：在神经网络中，Sigmoid 函数可以用作隐藏层的激活函数，尽管在现代深度学习中，ReLU 和其变种（如 LeakyReLU、ELU 等）更常用，因为它们可以缓解梯度消失问题。
   > 3. **损失函数**：在二分类问题中，通常与 Sigmoid 函数结合使用的是二元交叉熵损失函数（Binary Cross-Entropy Loss）。
   >
   > ### 总结
   >
   > - **Sigmoid 函数**：将实数域映射到 (0, 1) 区间内，适用于表示概率。
   > - **PyTorch 实现**：使用 `torch.sigmoid` 或 `F.sigmoid` 计算 Sigmoid 函数的值。
   > - **应用**：常用于二分类问题、神经网络和损失函数。
   >
   > 希望这些解释和示例能帮助你更好地理解 Sigmoid 函数及其在 PyTorch 中的使用。如果有更多问题，请随时提问。
   
   - ==**使用 `torch.log1p`**：计算 log⁡(1+x)log(1+*x*) 以避免直接计算 `log(0)` 的问题==。
   
10. 直接对预测分数排序，导致梯度中断，使用了==软排序softsort==

    > ==所谓梯度中断，是使用的运算不可微，导致无法反向传播（求偏导）==

    > 在深度学习中，排序操作通常会导致梯度中断，因为排序是一个非可导操作。为了在保持梯度传递的同时进行排序，可以考虑使用一些近似的可微方法。以下是几种常见的方法：
    >
    > #### 1. **软排序（SoftSort）**
    > 软排序是一种近似排序的方法，可以保持梯度传递。它通过构建一个软排序矩阵来近似排序操作。
    >
    > 软排序通过引入平滑的近似方法，使得排序操作变得可微，从而可以在梯度下降等优化算法中使用。
    >
    > ```
    > def softsort(x, tau=0.1):
    >     """
    >     输入向量 x 的形状为 [n]，则 x.unsqueeze(1) 的形状为 [n, 1]，
    >     x.unsqueeze(0) 的形状为 [1, n]。
    >     通过广播机制，x.unsqueeze(1) - x.unsqueeze(0) 会生成一个形状为 [n, n] 的矩阵，
    >     其中每个元素表示原向量中两个元素的差值。
    >     """
    >     # 计算每对元素的差值
    >     pairwise_diff = x.unsqueeze(1) - x.unsqueeze(0)
    >     """
    >     计算这些差值的绝对值，以确保相似度矩阵中的所有元素都是非负的。
    >     将这些绝对差值转换为相似度，我们取负值
    >     (将较大的差值转换为较小的相似度值。因为较大的差值表示两个元素相距较远，相似度较低。)
    >     并除以温度参数 tau。
    >     (除以温度参数 tau 是为了调整相似度的平滑程度。较小的 tau 值会使相似度矩阵中的值差异更大，
    >     从而更接近于硬排序；较大的 tau 值会使相似度矩阵中的值更加平滑，从而更接近于软排序。)
    >     温度参数 tau 控制着相似度矩阵的平滑程度。
    >     较小的 tau 值会使相似度矩阵更接近于硬排序（即 one-hot 编码），而较大的 tau 值会使相似度矩阵更加平滑
    >     """
    >     # 计算相似度矩阵
    >     similarity_matrix = -pairwise_diff.abs() / tau
    >     """
    >     使用 softmax 函数将相似度矩阵转换为一个软排序矩阵。
    >     这个矩阵的每一行表示一个元素在排序后的位置的概率分布。
    >     """
    >     # 计算软排序矩阵
    >     soft_permutation_matrix = F.softmax(similarity_matrix, dim=1)
    >     # 计算软排序后的张量
    >     soft_sorted_x = soft_permutation_matrix @ x
    >     return soft_sorted_x
    > ```
    >
    > #### 2. **Top-K 松弛操作**
    > Top-K 松弛操作通过引入温度参数来近似 Top-K 操作，使其可微。
    >
    > #### 3. **Gumbel-Softmax 重参数化**
    > Gumbel-Softmax 重参数化技巧可以用于近似离散操作，使其可微。
    >
    > #### 其他方法
    >
    > 1. **Top-K 松弛操作**：
    >    - 使用温度参数 `tau` 来近似 Top-K 操作。
    >    - 例如，可以使用 `F.softmax` 和 `topk` 函数来实现。
    >
    > 2. **Gumbel-Softmax 重参数化**：
    >    - 使用 Gumbel-Softmax 技巧来近似离散操作。
    >    - 适用于多分类问题中的离散选择。
    >
    > 希望这些方法能帮助你在保持梯度传递的同时进行排序操作。如果有更多问题，请随时提问。

11. ## Softmax

    > `softmax` 函数能够将一个向量转换为概率分布，并且在软排序中实现平滑的排序效果，其背后的原理可以从以下几个方面来理解：
    >
    > ### 1. **Softmax 函数的定义**
    >
    > `softmax` 函数的定义如下：
    >
    > $ \text{softmax}(\mathbf{x})_i = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}} $
    >
    > 其中，\(\mathbf{x}\) 是一个长度为 \(n\) 的向量，$\text{softmax}(\mathbf{x})_i$ 表示向量 \(\mathbf{x}\) 经过 `softmax` 函数变换后第 \(i\) 个元素的值。
    >
    > ### 2. **归一化**
    >
    > `softmax` 函数将每个元素 \(x_i\) 映射到一个范围在 \((0, 1)\) 之间的值，并且所有这些值的和为 1。这意味着 `softmax` 函数将向量 \(\mathbf{x}\) 转换为一个概率分布。
    >
    > ### 3. **温度参数 \($\tau$\)**
    >
    > 在软排序中，通常会引入一个温度参数 \(\tau\)，使得 `softmax` 函数的定义变为：
    >
    > $ \text{softmax}(\mathbf{x} / \tau)_i = \frac{e^{x_i / \tau}}{\sum_{j=1}^n e^{x_j / \tau}} $
    >
    > 温度参数 \(\tau\) 控制着 `softmax` 函数的平滑程度：
    >
    > - **当 \(\tau\) 较小时**：`softmax` 函数的结果更加尖锐，接近于 one-hot 编码。这是因为较小的 \(\tau\) 会使指数项之间的差异放大，导致较大的值占据主导地位，而其他值接近于 0。
    > - **当 \(\tau\) 较大时**：`softmax` 函数的结果更加平滑，每个值都比较接近。这是因为较大的 \(\tau\) 会使指数项之间的差异缩小，从而使每个值都有一定的概率。
    >
    > <img src=".\241012.assets\15e9c43bfa595f0829d8910f68afcf46.png" alt="img" style="zoom:50%;" />

12. ==对两种参数归一化==

    ```
    true_ranks = (true_ranks1 - torch.min(true_ranks1)) / (torch.max(true_ranks1) - torch.min(true_ranks1))
    ```

13. ```
    loss  /int(comb(len(scores), 2))
    ```

    会导致损失值不贴和吧，，，

14. 直接用==CrossEntropyLoss==

    epoch:0, loss:642.586181640625
    epoch:1, loss:655.1332397460938
    epoch:2, loss:539.5330200195312
    epoch:3, loss:460.69256591796875
    epoch:4, loss:450.5557861328125
    epoch:5, loss:431.3824157714844
    epoch:6, loss:425.2432556152344
    epoch:7, loss:400.6612548828125
    epoch:8, loss:380.50732421875
    epoch:9, loss:395.83489990234375
    epoch:10, loss:419.219482421875
    epoch:11, loss:393.0357971191406
    epoch:12, loss:368.60174560546875
    epoch:13, loss:369.1983947753906
    epoch:14, loss:366.9642028808594
    epoch:15, loss:374.33819580078125
    epoch:16, loss:363.66650390625
    epoch:17, loss:367.6988830566406
    epoch:18, loss:358.4563903808594
    epoch:19, loss:355.3465576171875
    epoch:20, loss:363.7801208496094
    epoch:21, loss:367.78167724609375
    epoch:22, loss:357.5622253417969
    epoch:23, loss:352.73077392578125
    epoch:24, loss:349.304931640625
    epoch:25, loss:343.16790771484375
    epoch:26, loss:356.6529541015625
    epoch:27, loss:339.1769714355469
    epoch:28, loss:333.3101806640625
    epoch:29, loss:334.16046142578125
    epoch:30, loss:324.69342041015625

    好像效果好一点：下降的更快

    （排序，排序）（relu）（CrossEntropyLoss）![_Training_Loss_epoch_40_lr_0.001_20241119_190046](.\241012.assets\_Training_Loss_epoch_40_lr_0.001_20241119_190046.svg)

    

15. 修改RegressionModule的激活函数relu为==leaky_relu==，很异常

    （排序，排序）（leaky_relu）（CrossEntropyLoss）![_Training_Loss_epoch_40_lr_0.001_20241119_190905](.\241012.assets\_Training_Loss_epoch_40_lr_0.001_20241119_190905.svg)

16. 

    > 损失函数呈现齿状下降（即损失值在下降过程中有明显的波动）是训练过程中常见的现象，这可能是由多种因素引起的。以下是一些可能的原因及相应的解决方法：
    >
    > #### 1. **学习率问题**
    > - **问题**：学习率可能设置得过高，导致梯度更新过大，从而使损失值在下降过程中出现波动。
    > - **解决方法**：
    >   - **降低学习率**：尝试逐步降低学习率，观察损失值的变化。
    >   - **使用学习率调度器**：使用学习率调度器（如 `torch.optim.lr_scheduler.ReduceLROnPlateau`）动态调整学习率，以便在训练过程中逐渐降低学习率。
    >
    > #### 2. **批量大小问题**
    > - **问题**：批量大小（batch size）可能设置得过小，导致每次更新的梯度噪声较大。
    > - **解决方法**：
    >   - **增加批量大小**：尝试增加批量大小，以减少梯度噪声，使损失值更加平滑。
    >
    > #### 3. **数据预处理问题**
    > - **问题**：数据可能没有很好地预处理，导致模型难以学习。
    > - **解决方法**：
    >   - **归一化或标准化**：确保输入数据经过归一化或标准化处理，使数据分布在合理范围内。
    >   - **检查数据分布**：确保数据分布合理，没有异常值或极端值。
    >
    > #### 4. **模型复杂度问题**
    > - **问题**：模型可能过于复杂或过于简单。
    > - **解决方法**：
    >   - **增加模型复杂度**：如果模型过于简单，可能无法捕捉数据的复杂性。可以增加隐藏层或增加神经元数量。
    >   - **减少模型复杂度**：如果模型过于复杂，可能会过拟合。可以减少隐藏层或减少神经元数量。
    >
    > #### 5. **正则化问题**
    > - **问题**：模型可能过拟合或欠拟合。
    > - **解决方法**：
    >   - **增加正则化**：使用 L1 或 L2 正则化来防止过拟合。
    >   - **减少正则化**：如果模型欠拟合，可以减少正则化强度。
    >
    > #### 6. **梯度问题**
    > - **问题**：可能存在梯度消失或梯度爆炸问题。
    > - **解决方法**：
    >   - **梯度裁剪**：使用梯度裁剪（`torch.nn.utils.clip_grad_norm_`）来防止梯度爆炸。
    >   - **使用更稳定的激活函数**：例如，使用 ReLU 的变体（如 LeakyReLU 或 PReLU）来缓解梯度消失问题。
    >
    > #### 7. **数据加载问题**
    > - **问题**：数据加载过程中可能存在随机性，导致每次迭代的数据顺序不同。
    > - **解决方法**：
    >   - **固定随机种子**：确保数据加载的随机性是固定的，可以使用 `torch.manual_seed` 和 `numpy.random.seed` 固定随机种子。
    >   - **数据增强**：如果使用数据增强，确保增强方式一致。
    >
    > #### 进一步调试
    >
    > 1. **检查数据分布**：确保数据分布合理，没有异常值或极端值。
    > 2. **增加正则化**：如果模型过拟合，可以尝试增加 L2 正则化。
    > 3. **调整模型结构**：尝试增加或减少隐藏层，调整神经元数量。
    > 4. **使用更稳定的激活函数**：例如，使用 LeakyReLU 或 PReLU。
    >
    > 希望这些方法能帮助你减少损失值的波动，使损失曲线更加平滑。如果有更多问题，请随时提问。

17. ==学习率调度器==尝试torch.optim.lr_scheduler.ReduceLROnPlateau

    （排序，排序）（relu）（CrossEntropyLoss）（学习率调度器patience=4）

    ![_Training_Loss_epoch_100_lr_0.001_20241119_204558](.\241012.assets\_Training_Loss_epoch_100_lr_0.001_20241119_204558.svg)

    确实可以让损失函数平稳一些，但是下降很缓慢，（lr变小后不能变大吗）

    （分数，分数）（relu）(loss)（学习率调度器）

    ![_Training_Loss_epoch_30_lr_0.01_20241119_210340](.\241012.assets\_Training_Loss_epoch_30_lr_0.01_20241119_210340.svg)

    ![_Training_Loss_epoch_70_lr_0.01_20241119_210004](.\241012.assets\_Training_Loss_epoch_70_lr_0.01_20241119_210004.svg)

    （排序，排序）（leaky_relu）（CrossEntropyLoss）（学习率调度器）

    ![_Training_Loss_epoch_30_lr_0.001_20241119_212920](.\241012.assets\_Training_Loss_epoch_30_lr_0.001_20241119_212920-1732023640912.svg)

    （排序，排序）（leaky_relu）（CrossEntropyLoss）（无学习率调度器）

    ![_Training_Loss_epoch_30_lr_0.001_20241119_214258](.\241012.assets\_Training_Loss_epoch_30_lr_0.001_20241119_214258.svg)

    

18. 反向传播在做什么

    损失值大小本身无所谓，重要在于损失函数的公式本身，是对学习参数的求导，所以重点在于预测值在损失函数中的形态

19. （分数，分数）（relu）（CrossEntropyLoss）（学习率调度器）

    ![_Training_Loss_epoch_30_lr_0.001_20241119_212920](D:\Tjnu-p\ML-learning\note\241012.assets\_Training_Loss_epoch_30_lr_0.001_20241119_212920.svg)

    （排序，排序）（relu）（CrossEntropyLoss）（无学习率调度器）

    ![_Training_Loss_epoch_30_lr_0.001_20241119_214900](.\241012.assets\_Training_Loss_epoch_30_lr_0.001_20241119_214900.svg)

    （排序，排序）（relu）（CrossEntropyLoss）（学习率调度器）

    ![_Training_Loss_epoch_30_lr_0.001_20241119_215547](.\241012.assets\_Training_Loss_epoch_30_lr_0.001_20241119_215547.svg)

    （排序，排序）（relu）（CrossEntropyLoss）（学习率调度器）

    ![_Training_Loss_epoch_70_lr_0.001_20241119_220100](.\241012.assets\_Training_Loss_epoch_70_lr_0.001_20241119_220100.svg)

    （排序，排序）（relu）（CrossEntropyLoss）（学习率调度器）

    ![_Training_Loss_epoch_70_lr_0.0001_20241119_222536](.\241012.assets\_Training_Loss_epoch_70_lr_0.0001_20241119_222536.svg)

20. 都进行软排序，不对劲

（排序，排序）（都软排序）（relu）（CrossEntropyLoss）（学习率调度器）

![_Training_Loss_epoch_70_lr_0.001_20241119_221458](.\241012.assets\_Training_Loss_epoch_70_lr_0.001_20241119_221458.svg)

（排序，排序）（都软排序）（relu）（CrossEntropyLoss）（学习率调度器）

![_Training_Loss_epoch_70_lr_0.0001_20241119_222030](.\241012.assets\_Training_Loss_epoch_70_lr_0.0001_20241119_222030.svg)

（排序，排序）（都软排序）（relu）（loss）（学习率调度器）

![_Training_Loss_epoch_70_lr_0.0001_20241119_223302](.\241012.assets\_Training_Loss_epoch_70_lr_0.0001_20241119_223302.svg)

（排序，排序）（都软排序）（relu）（CrossEntropyLoss）（学习率调度器）

![_Training_Loss_epoch_70_lr_1e-05_20241119_223856](.\241012.assets\_Training_Loss_epoch_70_lr_1e-05_20241119_223856-1732027881644.svg)

300epoch趋于稳定

![_Training_Loss_epoch_300_lr_1e-05_20241120_125413](.\241012.assets\_Training_Loss_epoch_300_lr_1e-05_20241120_125413.svg)

## 241118

1. 模型构筑好了但是不下降，有问题
   1. 损失函数
   2. 单个点还是所有点
   3. 优化是不是没有优化上
   4. 线性层到底在做什么，注意力机制又在做什么
   5. batch??
   6. cuda的也有问题

epoch_21_lr_0.001![_Training_Loss_epoch_21_lr_0.001_20241118_164639](.\241012.assets\_Training_Loss_epoch_21_lr_0.001_20241118_164639.svg)

修改了lr，lr太大，反而不能收敛

epoch_100_lr_1e-05![_Training_Loss_epoch_100_lr_1e-05_20241118_210319](.\241012.assets\_Training_Loss_epoch_100_lr_1e-05_20241118_210319.svg)

**loss = ranking_loss(scores, criticality_scores)**

需要大一点的epoch

epoch_40_lr_1e-05![_Training_Loss_epoch_40_lr_1e-05_20241118_210824](.\241012.assets\_Training_Loss_epoch_40_lr_1e-05_20241118_210824.svg)

![_Training_Loss_epoch_150_lr_0.0001_20241118_212115](.\241012.assets\_Training_Loss_epoch_150_lr_0.0001_20241118_212115.svg)

比用图阻值计算的损失更小，但是下降的很慢

epoch_500_lr_0.0001![_Training_Loss_epoch_500_lr_0.0001_20241118_215418](.\241012.assets\_Training_Loss_epoch_500_lr_0.0001_20241118_215418.svg)

>在训练深度学习模型时，如果损失值（loss）变为 `NaN`（Not a Number），这通常表示训练过程中出现了某种问题。`NaN` 值的出现可能是由多种原因引起的，以下是一些常见的原因及相应的解决方法：
>
>#### 1. **学习率过高**
>高学习率可能导致梯度爆炸，从而使损失值变为 `NaN`。
>
>**解决方案**：
>- **降低学习率**：尝试使用更小的学习率，例如从 0.01 降低到 0.001 或 0.0001。
>- **学习率调度**：使用学习率调度器（如 `torch.optim.lr_scheduler.StepLR` 或 `ReduceLROnPlateau`）在训练过程中逐渐降低学习率。
>
>#### 2. **梯度爆炸**
>梯度爆炸是指梯度值变得非常大，导致参数更新过大，从而使损失值变为 `NaN`。
>
>**解决方案**：
>- **梯度裁剪**：使用梯度裁剪技术（如 `torch.nn.utils.clip_grad_norm_`）来限制梯度的大小。
>
>```python
>import torch.nn.utils as utils
>
>for epoch in range(num_epochs):
>    for batch_idx, (data, target) in enumerate(train_loader):
>        optimizer.zero_grad()
>        output = model(data)
>        loss = criterion(output, target)
>        loss.backward()
>        
>        # 梯度裁剪
>        utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
>        
>        optimizer.step()
>```
>
>#### ~~3. **数据问题**~~
>训练数据中的异常值或 NaN 值可能导致损失值变为 `NaN`。
>
>**解决方案**：
>- **数据预处理**：对数据进行清洗，去除异常值和 NaN 值。
>- **数据标准化**：对输入数据进行标准化处理，使其均值为 0，方差为 1。
>
>```python
>import numpy as np
>
># 假设 input_data 和 target_data 是 numpy 数组
>input_data = np.nan_to_num(input_data)  # 将 NaN 值替换为 0
>target_data = np.nan_to_num(target_data)  # 将 NaN 值替换为 0
>```
>
>#### ~~4. **模型初始化问题**~~
>模型的初始权重可能不合适，导致训练过程中出现 `NaN`。
>
>**解决方案**：
>
>- **权重初始化**：使用合适的权重初始化方法，如 Xavier 或 He 初始化。
>
>```python
>import torch.nn.init as init
>
>def init_weights(m):
>    if isinstance(m, nn.Linear):
>        init.xavier_uniform_(m.weight)
>        if m.bias is not None:
>            init.zeros_(m.bias)
>
>model.apply(init_weights)
>```
>
>#### 5. **数值稳定性问题**
>某些操作（如除以非常小的数或对非常小的数取对数）可能导致数值稳定性问题，从而产生 `NaN`。
>
>**解决方案**：
>- **数值稳定性**：在计算中添加一个小的常数（如 `1e-8`）来避免除以零或取对数时出现问题。
>
>```python
># 例如，在计算对数时
>epsilon = 1e-8
>log_value = torch.log(value + epsilon)
>```
>
>#### 6. **激活函数问题**
>某些激活函数（如 ReLU）可能导致梯度消失或梯度爆炸，从而产生 `NaN`。
>
>**解决方案**：
>- **选择合适的激活函数**：使用 LeakyReLU 或 PReLU 等变体来替代 ReLU，以避免梯度消失问题。
>
>```python
>import torch.nn as nn
>
>model = nn.Sequential(
>    nn.Linear(10, 10),
>    nn.LeakyReLU(),
>    nn.Linear(10, 1)
>)
>```
>
>#### 7. **正则化问题**
>过强的正则化（如 `weight_decay` 过大）可能导致损失值变为 `NaN`。
>
>**解决方案**：
>- **调整正则化强度**：适当减小 `weight_decay` 的值。
>
>```python
>optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)
>```
>
>#### 8. **检查损失函数**
>损失函数的实现可能有误，导致计算出的损失值为 `NaN`。
>
>**解决方案**：
>- **检查损失函数**：确保损失函数的实现正确，没有除以零或取对数时出现问题。
>
>#### 9. **固定随机种子**
>训练过程中的随机性可能导致 `NaN` 的出现。
>
>**解决方案**：
>- **固定随机种子**：在训练开始时固定随机种子，以确保实验的可重复性。
>
>```python
>import random
>import numpy as np
>import torch
>
>seed = 42
>random.seed(seed)
>np.random.seed(seed)
>torch.manual_seed(seed)
>torch.cuda.manual_seed_all(seed)
>```
>
>#### 总结
>`NaN` 值的出现通常是多方面因素共同作用的结果。通过调整学习率、梯度裁剪、数据预处理、模型初始化、数值稳定性、激活函数、正则化强度等方法，可以有效地解决 `NaN` 问题。

## 241117

1. 发现mse_all的对比对象有问题

   修正后效果还是不好，只对填充后的值做MSE,未对填充后的值进行汇聚（多0值和全满矩阵之间的差距还是太大，需要进一步变化）

2. `enumerate` 在遍历列表、元组或其他可迭代对象时，同时获取元素及其对应的索引。这对于需要在循环中同时处理索引和值的情况非常方便。

3. 权重参数在模型中如何体现self.W？？？

   > 在数学公式中，权重参数通常表示模型中的可学习参数，这些参数在实际代码中是通过神经网络层中的权重矩阵来实现的。在 PyTorch 中，这些权重参数通常是在定义模型时自动初始化的，并在训练过程中通过反向传播和优化算法进行更新。
   >
   > #### 数学公式与代码的对应关系
   >
   > 假设我们有一个简单的线性模型 \( y = Wx + b \)，其中 \( W \) 是权重矩阵，\( x \) 是输入向量，\( b \) 是偏置项，\( y \) 是输出向量。在数学公式中，\( W \) 和 \( b \) 是显式表示的参数。
   >
   > 在 PyTorch 中，这个模型可以定义如下：
   >
   > ```python
   > import torch
   > import torch.nn as nn
   > 
   > class LinearModel(nn.Module):
   >     def __init__(self, input_dim, output_dim):
   >         super(LinearModel, self).__init__()
   >         self.linear = nn.Linear(input_dim, output_dim)
   >     
   >     def forward(self, x):
   >         return self.linear(x)
   > 
   > # 创建模型实例
   > input_dim = 10
   > output_dim = 1
   > model = LinearModel(input_dim, output_dim)
   > 
   > # 查看模型参数
   > for name, param in model.named_parameters():
   >     print(name, param)
   > ```
   >
   > #### 详细解释
   >
   > 1. **定义模型**：
   >    - `nn.Linear(input_dim, output_dim)` 创建一个线性层，其中 `input_dim` 是输入特征的维度，`output_dim` 是输出特征的维度。
   >    - 这个线性层内部自动初始化了权重矩阵 \( W \) 和偏置项 \( b \)。
   >
   > 2. **前向传播**：
   >    - `self.linear(x)` 在前向传播过程中计算 \( y = Wx + b \)。
   >
   > 3. **查看模型参数**：
   >    - `model.named_parameters()` 返回模型中所有可学习参数的名称和值。
   >    - 通常，线性层的权重矩阵会被命名为 `linear.weight`，偏置项会被命名为 `linear.bias`。
   >
   > #### 例子：注意力机制
   >
   > 假设我们有一个简单的注意力机制，其数学公式如下：
   >
   > $ \text{attention}(q, k, v) = \text{softmax}\left(\frac{qk^T}{\sqrt{d_k}}\right)v$
   >
   > 其中 \( q \) 是查询向量，\( k \) 是键向量，\( v \) 是值向量，\( d_k \) 是键向量的维度。
   >
   > 在 PyTorch 中，这个注意力机制可以实现如下：
   >
   > ```python
   > import torch
   > import torch.nn as nn
   > import torch.nn.functional as F
   > 
   > class AttentionLayer(nn.Module):
   >     def __init__(self, input_dim, hidden_dim):
   >         super(AttentionLayer, self).__init__()
   >         self.W_q = nn.Linear(input_dim, hidden_dim, bias=False)
   >         self.W_k = nn.Linear(input_dim, hidden_dim, bias=False)
   >         self.W_v = nn.Linear(input_dim, hidden_dim, bias=False)
   >     
   >     def forward(self, query, key, value):
   >         Q = self.W_q(query)
   >         K = self.W_k(key)
   >         V = self.W_v(value)
   >         
   >         scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(K.size(-1), dtype=torch.float32))
   >         attention_weights = F.softmax(scores, dim=-1)
   >         weighted_sum = torch.matmul(attention_weights, V)
   >         
   >         return weighted_sum
   > 
   > # 创建模型实例
   > input_dim = 10
   > hidden_dim = 10
   > attention_layer = AttentionLayer(input_dim, hidden_dim)
   > 
   > # 创建输入张量
   > query = torch.randn(1, 10)
   > key = torch.randn(5, 10)
   > value = torch.randn(5, 10)
   > 
   > # 前向传播
   > output = attention_layer(query, key, value)
   > 
   > print("输出张量:", output)
   > print("输出张量形状:", output.shape)
   > ```
   >
   > #### 详细解释
   >
   > 1. **定义注意力层**：
   >    - `self.W_q`、`self.W_k` 和 `self.W_v` 是三个线性层，分别用于计算查询向量 \( q \)、键向量 \( k \) 和值向量 \( v \) 的变换。
   >    - 这些线性层内部自动初始化了权重矩阵。
   >
   > 2. **前向传播**：
   >    - `Q = self.W_q(query)` 计算查询向量的变换。
   >    - `K = self.W_k(key)` 计算键向量的变换。
   >    - `V = self.W_v(value)` 计算值向量的变换。
   >    - `scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(K.size(-1), dtype=torch.float32))` 计算注意力分数。
   >    - `attention_weights = F.softmax(scores, dim=-1)` 计算注意力权重。
   >    - `weighted_sum = torch.matmul(attention_weights, V)` 计算加权求和的值向量。
   >
   > 3. **查看模型参数**：
   >    - `attention_layer.named_parameters()` 返回注意力层中所有可学习参数的名称和值。
   >
   > #### 总结
   >
   > - **数学公式中的权重参数**：在数学公式中，权重参数是显式表示的。
   > - **代码中的权重参数**：在 PyTorch 中，这些权重参数通过神经网络层（如 `nn.Linear`）自动初始化，并在训练过程中通过反向传播和优化算法进行更新。
   > - **对应关系**：数学公式中的权重参数在代码中通过神经网络层的权重矩阵来实现。
   >

4. torch.cat 是 PyTorch 中的一个函数，用于在指定维度上拼接多个张量。通过设置 dim 参数，您可以控制在哪个维度上进行拼接。dim 参数指定了拼接操作的轴或维度。

5. ![Figure_1](.\241012.assets\Figure_1-1731853399243.png)

   每个节点的邻居个数不同

   邻居节点的特征汇聚，维度如何设定，根据哪个维度进行汇聚，应该2000汇聚，那么怎么处理多个邻居

## 241115

1. 沿途小师

   和人工智能长期对话，会感觉他的语料缺乏创新性，或者说像是函数进入平稳期，会翻来覆去重复上下文，导致逐渐混乱、出错，需要重开一个对话。

   对话的创新性和可靠性最高点存在于对话过程中的某一个阶段，比如一开始，新颖，但是不够懂我们（我们提供的语料不够），几个轮回后充分理解我们的意思，这个阶段最准确，但是再往后，就翻来覆去缺乏创新甚至逻辑有误了。

   > 显示latex公式、代码不稳定
   >
   > 不能够复制latex公式

## 241114

==计划，梳理==

1. 计算残差图（G−n）的鲁棒性度量
2. 



## 241113 性能指标选点结果

==计划==

1. ~~梳理一下公式~~，和弹性训练的公式
2. ~~画线界定一下区间~~，做一下后面弹性的训练
3. 再回来处理性能指标的问题



1. 在 LaTeX 中表示乘法有几种常见的方法，取决于你希望乘法符号如何显示。以下是几种常用的方式：

   1. **使用星号(*)**：
      - 这是最简单的形式，但在数学公式中通常不推荐使用，因为它看起来像编程语言中的乘法操作符。
      - 示例代码：`$a * b$`
      - 显示结果：\(a * b\)

   2. **使用点乘(\cdot)**：
      - 这是数学文献中最常用的乘法表示方式。
      - 示例代码：`$a \cdot b$`
      - 显示结果：\(a \cdot b\)

   3. **使用叉乘(\times)**：
      - 当需要明确表示向量或矩阵的乘法时，通常使用叉乘。
      - 示例代码：`$a \times b$`
      - 显示结果：\(a \times b\)

   4. **直接相邻**：
      - 在某些情况下，特别是在代数表达式中，两个变量或数字直接相邻就表示乘法。
      - 示例代码：`$ab$`
      - 显示结果：\(ab\)

   5. **使用括号**：
      - 有时为了清晰起见，会用括号来表示乘法。
      - 示例代码：`$(a)(b)$`
      - 显示结果：\((a)(b)\)

   选择哪种方式取决于具体的上下文和个人偏好。在正式的数学文档中，使用 `\cdot` 或者直接相邻的形式最为常见。

2. **计算两两之间的距离**：

   - 使用 `pdist` 函数计算每对点之间的距离，得到一个一维数组。
   - `pdist` 函数默认使用欧几里得距离（Euclidean distance），但也可以指定其他距离度量。

3. **将一维数组转换为二维矩阵**：

   - 使用 `squareform` 函数将 `pdist` 返回的一维数组转换为一个 n×n 的对称矩阵。
   
4. - **`min`**：返回的是最小值本身。
   - **`argmin`**：返回的是使函数取得最小值的输入参数。

   1. `min`（最小值）

   `min` 表示一个集合或函数的最小值。具体来说，`min` 返回的是最小的那个值本身。

   2. `argmin`（最小值点）

   `argmin` 表示使某个函数取得最小值的输入参数。具体来说，`argmin` 返回的是使函数值最小的那个输入值或输入值的集合。

   对于一个函数 \( f(x) \)，`argmin` 表示使函数取得最小值的输入参数：

   $\arg\min_{x \in D} f(x)$

   其中 \( D \) 是函数 \( f(x) \) 的定义域。

5. 剩余平均能量逻辑有误

   完全重构AEC,效果还凑合

   ![select-indicators](.\241012.assets\select-indicators.svg)

![select-indicators2](.\241012.assets\select-indicators2.svg)

![select-indicators3](.\241012.assets\select-indicators3.svg)

![Gp75_MGC-RMselect14](.\241012.assets\Gp75_MGC-RMselect14.svg)

==感觉有点夸张，减少的是不是太小了点，没有测试图级的特征==

## 241112  填充0值

1. readout

- `self.weight`：一个可学习的权重矩阵，形状为 `[output_size, output_size]`。这个权重矩阵用于转换全局特征。

- `nn.init.xavier_uniform_`：使用 Xavier 均匀分布初始化权重矩阵，这是一种常用的权重初始化方法，有助于加速训练过程。

- **输入**：

  - `h`：节点特征矩阵，形状为 `[num_nodes, feature_dim]`，其中 `num_nodes` 是节点的数量，`feature_dim` 是每个节点的特征维度。

- **步骤解析**：

  1. **计算全局特征的初始值**：

     ```
global_h = torch.matmul(torch.mean(h, dim=0), self.weight)
     ```
     
     - `torch.mean(h, dim=0)`：计算所有节点特征的平均值，结果是一个形状为 `[feature_dim]` 的向量。
- `torch.matmul(..., self.weight)`：将平均特征与权重矩阵相乘，得到一个形状为 `[output_size]` 的向量 `global_h`。
  
  2. **转换全局特征**：

     ```
transformed_global = torch.tanh(global_h)
     ```
  
     - `torch.tanh`：使用双曲正切函数对 `global_h` 进行非线性变换，结果仍然是一个形状为 `[output_size]` 的向量 `transformed_global`。
  
  3. **计算注意力得分**：

     ```
sigmoid_scores = torch.sigmoid(torch.mm(h, transformed_global.view(-1, 1)).sum(dim=1))
     ```
  
     - `transformed_global.view(-1, 1)`：将 `transformed_global` 转换为形状为 `[output_size, 1]` 的列向量。
- `torch.mm(h, ...)`：将节点特征矩阵 `h` 与 `transformed_global` 相乘，结果是一个形状为 `[num_nodes, 1]` 的矩阵。
     - `.sum(dim=1)`：对每个节点的乘积结果求和，得到一个形状为 `[num_nodes]` 的向量。
     - `torch.sigmoid(...)`：使用 Sigmoid 函数将每个节点的得分转换为 [0, 1] 范围内的注意力权重 `sigmoid_scores`。
     
4. **加权节点特征**：
  
     ```
    h_global = sigmoid_scores.unsqueeze(-1) * h
    ```
   
     - `sigmoid_scores.unsqueeze(-1)`：将 `sigmoid_scores` 转换为形状为 `[num_nodes, 1]` 的列向量。
- `* h`：将注意力权重与节点特征矩阵相乘，结果是一个形状为 `[num_nodes, feature_dim]` 的矩阵 `h_global`，其中每个节点的特征都被相应的注意力权重加权。
  
5. **生成图级别的特征**：
  
     ```
    return h_global.sum(dim=0).unsqueeze(0)
   ```
   
     - `h_global.sum(dim=0)`：对所有节点的加权特征进行求和，结果是一个形状为 `[feature_dim]` 的向量。
- `.unsqueeze(0)`：在第0维增加一个新的维度，将结果转换为形状为 `[1, feature_dim]` 的张量，表示图级别的特征。

2. 使用了`zip()`函数来将这两个列表的对应元素配对，然后在每次循环中将这些配对的元素分别赋值给变量`low_dim_vector`和`node_list`

3. `enumerate(node_list)` 是一个非常有用的Python内置函数，它允许你在遍历列表（或其他可迭代对象）的同时获取每个元素的索引。这对于需要同时使用元素及其位置的情况特别有用。

4. 统一不同维度图的特征：全图节点特征构成图的特征，根据节点索引填充0同一纬度，在进行汇聚，但是效果不明显。

   > 在处理不同维度的向量时，直接计算均方误差（MSE）可能会遇到维度不匹配的问题。为了解决这个问题，可以采取以下几种方法之一：
   >
   > 1. **Padding（填充）**：将所有向量填充到相同的维度。
   > 2. **Truncation（截断）**：将所有向量截断到相同的维度。
   > 3. **Projection（投影）**：将所有向量投影到一个共同的低维空间。
   > 4. **Weighted MSE（加权MSE）**：根据向量的实际长度调整MSE的权重。
   >
   > ### 方法 1: Padding（填充）
   >
   > 将所有向量填充到相同的最大维度，通常用零填充。
   >
   > ```python
   > import torch
   > import torch.nn.functional as F
   > 
   > def pad_tensors(tensors, max_len):
   >     padded_tensors = []
   >     for tensor in tensors:
   >         padded_tensor = F.pad(tensor, (0, max_len - tensor.size(0)), 'constant', 0)
   >         padded_tensors.append(padded_tensor)
   >     return torch.stack(padded_tensors)
   > 
   > def mse_with_padding(pred, target):
   >     max_len = max(pred.size(0), target.size(0))
   >     pred_padded = pad_tensors([pred], max_len)[0]
   >     target_padded = pad_tensors([target], max_len)[0]
   >     return F.mse_loss(pred_padded, target_padded)
   > 
   > # 示例
   > pred = torch.tensor([1.0, 2.0, 3.0])
   > target = torch.tensor([1.0, 2.0, 3.0, 4.0])
   > mse = mse_with_padding(pred, target)
   > print(f'MSE with padding: {mse.item()}')
   > ```
   >
   > ### 方法 2: Truncation（截断）
   >
   > 将所有向量截断到相同的最小维度。
   >
   > ```python
   > def truncate_tensors(tensors, min_len):
   >     truncated_tensors = []
   >     for tensor in tensors:
   >         truncated_tensor = tensor[:min_len]
   >         truncated_tensors.append(truncated_tensor)
   >     return torch.stack(truncated_tensors)
   > 
   > def mse_with_truncation(pred, target):
   >     min_len = min(pred.size(0), target.size(0))
   >     pred_truncated = truncate_tensors([pred], min_len)[0]
   >     target_truncated = truncate_tensors([target], min_len)[0]
   >     return F.mse_loss(pred_truncated, target_truncated)
   > 
   > # 示例
   > pred = torch.tensor([1.0, 2.0, 3.0, 4.0])
   > target = torch.tensor([1.0, 2.0, 3.0])
   > mse = mse_with_truncation(pred, target)
   > print(f'MSE with truncation: {mse.item()}')
   > ```
   >
   > ### 方法 3: Projection（投影）
   >
   > 将所有向量投影到一个共同的低维空间。
   >
   > ```python
   > import torch
   > import torch.nn as nn
   > 
   > class ProjectionLayer(nn.Module):
   >     def __init__(self, input_size, output_size):
   >         super(ProjectionLayer, self).__init__()
   >         self.fc = nn.Linear(input_size, output_size)
   >     
   >     def forward(self, x):
   >         return self.fc(x)
   > 
   > def mse_with_projection(pred, target, proj_layer):
   >     pred_proj = proj_layer(pred.unsqueeze(0)).squeeze(0)
   >     target_proj = proj_layer(target.unsqueeze(0)).squeeze(0)
   >     return F.mse_loss(pred_proj, target_proj)
   > 
   > # 示例
   > input_size = 4
   > output_size = 3
   > proj_layer = ProjectionLayer(input_size, output_size)
   > 
   > pred = torch.tensor([1.0, 2.0, 3.0, 4.0])
   > target = torch.tensor([1.0, 2.0, 3.0])
   > mse = mse_with_projection(pred, target, proj_layer)
   > print(f'MSE with projection: {mse.item()}')
   > ```
   >
   > ### 方法 4: Weighted MSE（加权MSE）
   >
   > 根据向量的实际长度调整MSE的权重。
   >
   > ```python
   > def weighted_mse(pred, target):
   >     min_len = min(pred.size(0), target.size(0))
   >     pred_truncated = pred[:min_len]
   >     target_truncated = target[:min_len]
   >     mse = F.mse_loss(pred_truncated, target_truncated)
   >     
   >     # 计算剩余部分的MSE
   >     if pred.size(0) > min_len:
   >         mse += F.mse_loss(pred[min_len:], torch.zeros(pred.size(0) - min_len))
   >     if target.size(0) > min_len:
   >         mse += F.mse_loss(target[min_len:], torch.zeros(target.size(0) - min_len))
   >     
   >     return mse
   > 
   > # 示例
   > pred = torch.tensor([1.0, 2.0, 3.0, 4.0])
   > target = torch.tensor([1.0, 2.0, 3.0])
   > mse = weighted_mse(pred, target)
   > print(f'Weighted MSE: {mse.item()}')
   > ```
   >
   > ### 总结
   >
   > 以上方法都可以解决不同维度向量之间的MSE计算问题。选择哪种方法取决于具体的应用场景和数据特性：
   >
   > - **Padding**：适用于需要保留所有信息的情况。
   > - **Truncation**：适用于只需要比较共同部分的情况。
   > - **Projection**：适用于需要将高维数据降维的情况。
   > - **Weighted MSE**：适用于需要平衡不同长度向量的情况。

5. mse all ，填充0进行全图比较，太，，，，

   ![mse_all](.\241012.assets\mse_all-1731387690715.svg)

   还是得汇聚图级特征，或者自编码器

   但是还是得重新训练

6. ==有没有一种可能性就是，不可能真的只用2、3个传感器，所以前几个传感器的值很奇怪不用很纠结==

7. 画图随时记录

## 241111

1. Ds尝试了很多种算法

   Ds： 原始算法，没有细看f_distance和g_distance的原理，算法太慢了跑不出来

   Ds2:  修改算法对Rst和rmax的计算，未考虑物理距离

   Ds3：考虑物理距离，尝试了考虑连接的点数和没有考虑连接点数，区别不大

   ==先这样吧，有一个大概的趋势==

2. 考虑汇总成图级特征进行比较：MSE  MI

   1. MSE:

      提炼一个图级特征

      > ### 加性注意力机制
      >
      > 加性注意力机制（$Additive Attention Mechanism$），也被称为Bahdanau注意力或基于内容的注意力模型，是深度学习中一种重要的机制，用于在**处理序列数据时给予不同部分不同的权重**。这种机制最初是在神经机器翻译任务中提出并应用的，由Dzmitry Bahdanau等人在2015年的论文《Neural Machine Translation by Jointly Learning to Align and Translate》中介绍。
      >
      > #### 工作原理
      >
      > 加性注意力机制的核心思想是通过计算查询向量（query vector）与每个键向量（key vector）之间的相似度得分来决定给定序列中每个元素的重要性。这里的查询向量可以理解为解码器的状态，而键向量则是编码器产生的每个时间步的状态。具体来说，该机制通过一个前馈神经网络来计算这些得分，这个过程可以形式化为以下步骤：
      >
      > 1. **计算得分**：对于解码器状态\($s_t$\)和编码器每个时间步的状态\($h_i$\)，使用一个前馈神经网络（通常是一个单层的全连接网络）来计算得分\($e_{ij}$\)。这个得分反映了\($s_t$\)对\($h_i$\)的关注程度。数学上，可以表示为：
      >     $e_{ij} = v^T \tanh(W_s s_t + W_h h_i) 
      >    $其中，\($v$\)、\($W_s$\)和\($W_h$\)是可学习的参数，\($\tanh$\)是非线性激活函数。
      >
      > 2. **归一化得分**：将得分转换成概率分布，这一步通常是通过softmax函数实现的，以确保所有得分加起来等于1。这样就得到了注意力权重\($a_{ij}$\)：
      >    $a_{ij} = \frac{\exp(e_{ij})}{\sum_k \exp(e_{ik})}$
      >
      > 3. **计算上下文向量**：根据上述得到的注意力权重，对编码器的所有隐藏状态进行加权求和，从而获得上下文向量\($c_t$\)：
      >    $c_t = \sum_i a_{ij} h_i$
      >
      > 4. **使用上下文向量**：最后，将上下文向量\($c_t$\)与解码器的状态结合，用于生成下一个时间步的输出。
      >
      > #### 应用场景
      >
      > 加性注意力机制广泛应用于自然语言处理领域，包括但不限于：
      >
      > - **神经机器翻译**：帮助模型更好地理解源语言句子中的各个部分，并准确地将其翻译成目标语言。
      > - **文本摘要**：从长篇文章中提取关键信息，生成简洁的摘要。
      > - **问答系统**：提高模型对问题的理解能力，从而提供更准确的答案。
      >
      > 加性注意力机制通过允许模型动态地关注输入序列的不同部分，极大地提高了模型处理复杂任务的能力。然而，值得注意的是，随着输入序列长度的增长，计算注意力权重的成本也会相应增加，这在实际应用中可能需要考虑优化策略来缓解这一问题。

      > ### 多层感知机
      >
      > 多层感知机（Multilayer Perceptron, MLP）是一种前馈型人工神经网络模型，它将多个神经元组织成多层结构，包括一个输入层、一个或多个隐藏层以及一个输出层。每一层中的神经元都与下一层中的所有神经元相连，但同一层内的神经元之间没有连接。MLP能够学习非线性函数映射，因此在解决复杂的**分类和回归问题**上表现出色。
      >
      > #### 构成
      >
      > - **输入层**：接收原始数据输入。每个神经元对应输入特征的一个维度。
      > - **隐藏层**：位于输入层和输出层之间，可以有一个或多个。隐藏层中的每个神经元都会对来自前一层的输入进行**加权求和**，并通过一个激活函数来产生输出。
      > - **输出层**：根据任务需求设计，例如在二分类问题中，输出层可能只有一个神经元；而在多分类问题中，则可能有多个神经元，每个神经元对应一个类别。
      >
      > #### 激活函数
      >
      > 激活函数用于引入非线性，使得网络能够拟合复杂的函数关系。常见的激活函数包括：
      >
      > - **Sigmoid**：\(\sigma(x) = \frac{1}{1 + e^{-x}}\)，输出范围(0,1)，适合用于二分类问题的输出层。
      > - **ReLU (Rectified Linear Unit)**：\(f(x) = \max(0, x)\)，广泛用于隐藏层，因为它有助于缓解梯度消失问题。
      > - **Tanh (Hyperbolic Tangent)**：\(\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\)，输出范围(-1,1)，也是一种常用的非线性激活函数。
      > - **Softmax**：常用于多分类问题的输出层，可以将输出转化为概率分布。
      >
      > #### 训练过程
      >
      > MLP的训练通常采用反向传播算法来最小化损失函数。这个过程涉及以下几个步骤：
      >
      > 1. **前向传播**：从输入层开始，数据通过每一层的神经元传递到输出层，每经过一个神经元，都会进行加权求和并通过激活函数。
      > 2. **计算损失**：在输出层，根据预测值与真实值之间的差异计算损失函数的值。
      > 3. **反向传播**：从输出层向输入层方向，逐层计算每个神经元对最终损失的影响（即梯度），并据此调整各层的权重和偏置。
      > 4. **权重更新**：利用梯度下降等优化算法，根据计算出的梯度更新网络中的权重和偏置，以减小损失函数的值。
      >
      > #### 应用
      >
      > MLP广泛应用于各种领域，包括但不限于：
      >
      > - **图像识别**：通过训练MLP模型来识别图片中的对象。
      > - **语音识别**：将音频信号转换为文本。
      > - **自然语言处理**：如情感分析、文本分类等任务。
      > - **推荐系统**：根据用户的历史行为预测其可能感兴趣的商品或内容。
      >
      > 总之，多层感知机作为一种基础且强大的神经网络模型，在许多机器学习任务中都有着广泛的应用。

## 241110

1. 计算通信损失时没有考虑物理距离（自然连通性也没有考虑物理距离），==考虑物理距离==赋值为边的权重

   假设暂时不考虑物理距离

2. 网络生命会快速下降![_Reference indicator_test](.\241012.assets\_Reference indicator_test-1731226391354.svg)

​    剩余能量这个评估指标不好，换一个

3. 图核
4. 图嵌入
5. 得到的一个单一节点的特征表示对比

![_Reference indicator_test](.\241012.assets\_Reference indicator_test-1731234062188.svg)

![single_node_mse](.\241012.assets\single_node_mse.svg)

6. Ds的max_radius  没有考虑物理距离

   最新算法重新计算了$r_{max}$,相当于重新构图，没有按照原来的方法构图，

==**计划：**==

1. 考虑汇总成图级特征进行比较：MSE  MI
2. 考虑一些弹性方向的度量

## 241106

> 1. 指标的选择：多个不同维度的图进行比较
>    1. 节点特征聚合为图特征进行衡量：图特征聚合器训练
>    2. 比较过程中是否固定图的维度。（选出的节点重新构图/未选节点从原图中移除边、掩蔽特征）

1. 使用 NetworkX 库计算图的半径时，如果图不是连通的，nx.radius 函数会抛出 NetworkXError，因为半径的定义要求图必须是连通的。对于非连通图，你需要分别计算每个连通子图的半径。

   ![Figure_1](.\241012.assets\Figure_1-1730880573836.png)

   ![_Reference indicator](.\241012.assets\_Reference indicator.svg)

2. 平均剩余能量是什么含义,代码的含义

   > 1. **通信能耗模型**：定义了`communication_energy_loss`函数，用于计算单个节点与它的邻居进行一轮通信后的能量损失。这个函数考虑了两个主要因素：
   >
   >    - `E_elec`：电子设备消耗的能量。
   >    - `beta * dis ** alpha`：信号传输的能量损耗，其中`dis`是两节点之间的距离，`alpha`和`beta`是与传播介质相关的系数。
   >
   > 2. **网络生命周期计算**：在`network_life`函数中，通过循环模拟多轮通信过程。对于每一轮，==每个节点都会与其所有在通信范围内的邻居进行通信==（**那图上的连接关系是什么含义？？？连接如果是通信的含义，那就不应该是图半径内的点通信，而是相连的点通信**），并根据`communication_energy_loss`函数更新其能量值。当任意节点的能量降至0或以下时，该轮次结束，网络被认为失效。此时，计算剩余节点的平均能量值，并返回网络的总生存轮次和剩余平均能量。
   >
   >    **这个值主要和传感器数量和位置有关吧（在不在“通信范围内”）**

   改变通信范围后图像变化了

   ![Figure_1](.\241012.assets\Figure_1-1730900928648.png)

   ![Figure_2](.\241012.assets\Figure_2-1730902525657.png)

   

**==计划==**：

1. 平均剩余能量，
   1. 非连通的图半径能那样求吗
2. MSE等

## 241105

1. ![image-20241105151541366](.\241012.assets\image-20241105151541366.png)

   MSE衡量某个节点的预测特征与原始特征的均方误差？

   怎么用来评估整个图？

   对于全图而言的特征是？？拓扑特征？节点特征？

   什么意思，选出来的点通过特征预测后的特征与原特征的差距？

   **数据重建**

2. ![image-20241105151611491](.\241012.assets\image-20241105151611491.png)

   ![image-20241105151625072](.\241012.assets\image-20241105151625072.png)

3. ![image-20241105151726107](.\241012.assets\image-20241105151726107.png)

4. `.twinx()` 方法用于在同一张图中绘制两个具有不同 y 轴刻度的曲线。`.twinx()` 方法会创建一个新的 Axes 对象，共享同一个 x 轴，但有两个不同的 y 轴。

5. `.get_label()` 方法用于获取由 `.plot()` 或其他绘图方法设置的图例标签。这些标签通常用于图例（legend）来标识图表中的不同数据系列。

6. `axhline()` 方法用于在图表中绘制一条水平线。这条水平线可以用来标记特定的 y 值，例如平均值、阈值等。

7. `.legend()` 方法用于添加图例

8. 传感器覆盖面积作为衡量？

9. KL散度里的概率分布要怎么获得:
   连续特征：对于连续特征，可以使用直方图或核密度估计（KDE）来估计概率分布。

   嵌入向量：使用图嵌入方法（如 Node2Vec、GraphSAGE 等）生成节点的嵌入向量，可以计算嵌入向量的分布。
   
   ```python
   kl_divergence = entropy(degree_freq_G1, degree_freq_G2)
   ```
   
10. 自然连通性（Natural Connectivity）是一种衡量图的鲁棒性的指标，它反映了图在面对节点或边的删除时保持连通性的能力。自然连通性越高，图的鲁棒性越好。

11. ```
    find_value_according_index_list(aim_list, index_list):  # 索引转换 从排序索引找对应节点
    ```

    但是aim_list 是？节点特征，position 

12. 图鲁棒性归纳学习器Inductive Learner for Graph Robustness (ILGR)

13. 需要把不同数量节点的图构建出来

14. 按照重要性排序删点及其边，但是维度不变？这样会影响比较吧，在一些孤立点上会有影响吧

15. 确保不会出现“list assignment index out of range”错误。特别是，selected_node 列表的初始化应该在读取文件并填充 node_list 之后进行，以确保 selected_node 有足够的空间来存储每个 select_node 的值。

==**计划**==

1. 指标的分析：
   1. 自然连通性具体代表什么含义(√)
   2. 还有哪些可用的指标，针对优化和鲁棒性
2. 指标的代码构建
3. 弹性部分的展开逻辑

## 241104

1. gat权值共享怎么体现

2. 马尔科夫链？

3. > 1. **`np.sum(A, axis=1)`**:
   >    - 这个函数计算矩阵 A*A* 每一行的元素之和。结果是一个一维数组，其中每个元素对应于 A*A* 中相应行的元素总和。
   > 2. **`np.power(..., -1)`**:
   >    - 这个函数将上一步得到的一维数组中的每个元素取倒数（即，对每个元素应用幂 -1）。如果某行的和为0，则该操作会导致一个除以零的错误，因此在实际使用时需要确保没有行和为0，或者适当地处理这种异常情况。
   > 3. **`np.diag(...)`**:
   >    - 最后，`np.diag` 函数用于从上述步骤的结果创建一个对角矩阵。这意味着它会将输入的一维数组转换成一个二维数组，其中输入数组的元素位于矩阵的主对角线上，而所有其他位置的值都为0。
   > 4.  `np.dot(A, D)` 可以计算这两个矩阵的点积（矩阵乘法）。具体来说，这个操作将矩阵 *A* 与对角矩阵 *D* 相乘，其中 D*D* 的对角线元素是 A*A* 每一行元素之和的倒数。
   > 5. `np.ones(N)` 是 NumPy 库中的一个函数，用于创建一个包含全1的数组。具体来说，这个函数接受一个整数 `N` 作为参数，并返回一个长度为 `N` 的一维数组，其中所有元素都是1。
   > 6. `np.argsort` 是 NumPy 库中的一个函数，用于返回数组元素排序后的索引。具体来说，它返回一个索引数组，这些索引按照原数组中元素的排序顺序排列。

4. 相似度分数的异常值不影响排序

5. 整理师兄的评估指标代码，绘制评估图的性能

## 241103

> 早上一来，有图没数据，机器好像重启了，果然不能寄希望于没有被看着的机子
>
> 好在我电脑上的数据保住了，我的宝贝电脑此刻价值得到了极大的发挥
>
> 服务器的GPU价值好还是很大的

1. 看起来prediction 误差很小，但是similarity score之间的误差也很小，这样的误差足以变换次序了

2. **预测误差为什么是一致的**，在特定点上有一致的误差

3. 328异常  1.464528177166357636e-04 （收集数据）

   但是要用data[329]的数据修正data[328]

   ==**为什么不同数据，异常点不同？**==

4. colorbar?

5. 预测损失的规律性，与重要性得分之间存在什么关联？似乎是因此而影响的![Figure_1](.\241012.assets\Figure_1.png)

**计划**

1. 细看论文，解析代码

> prediction  loss  的分布情况，与节点的重要性相关：更为重要的节点在全部节点中占少数，模型预测对这些节点或其邻居节点表现出更大的误差；重要性较低的节点在全部节点中占比较大，在模型预测中表现出较小的误差。

## 241102 

GPU确实训练部分很快！！

但是模型评估部分运行特别**慢**

1. cuGraphy缺少wheel，安装了Docker，但不知道方法是否正确，还缺少什么环境

2. 最慢的部分不是math.exp，是NX.graph_edit_distance, 但是没有找到NX在cuda的办法

3. ==cuda的编程使用是个待研究的课题==

4. 每次获得全图的数据都很慢，一个小失误，一天白干。**注意小数据测试**

   ```python
   for i in range(perturbed_a.shape[0]-445):
   ```

5. 还是我的电脑跑的快一点

6. GPU同一个机器跑出来还会有不同？==GPU是不是需要单独设置种子==，越跑越平滑了，但是预测分数依然总是1

prediction loss 较低的点会不会训练的模型更好？230 75

prediction loss 较大的点拿来训练似乎会出现尖刺。但是每次会有不同。**预测分数也总是1**

![GraphPair_100_n450_d2000_Training_Loss_epoch_21_lr_0.0001_20241102_213748](.\241012.assets\GraphPair_100_n450_d2000_Training_Loss_epoch_21_lr_0.0001_20241102_213748.svg)

![image-20241102214256919](.\241012.assets\image-20241102214256919.png)

==**计划**：==

1. 对分数做处理，画出分数的彩色图
2. pagerank

## 241101

长时间休眠和睡眠，期间打开主机进行打印，然后系统崩溃，修复系统时文件损坏，不能修复，于是重装了系统。

1. 写readme文件

在服务器重新安装了pytorch，可以运行cuda，但是好像还是很慢

==先用效果好的小图导出模型，计算数值往后做==

## 241025

1. ==仅用一个图对训练出模型==
   
   1. 不需要训练的数据怎么快速通过模型获得结果
   
2. 他的模型没有考虑每个扰动图之间的关系
   
3. 图对能够把所有扰动图考虑进去吗？
   
4. 不同重要性节点的图对训练的模型有什么不同
   
2. loss有==尖刺==，（存在离群点干扰？），是否跟设备有关？

3. ==怎么调用已有的模型（√）==，需要forward吗，好像不用

4. ==测试集对比（数据量太大，不易实现，先不管了）==：
   
   1. 某扰动图模型的泛化能力，在其他扰动图上的损失效果： **代码变量调用可能有问题**
   
   2. ==大图（效果不好，未知问题，放置）==，改变节点后模型的能力： 模型中间的层需要更改??
   
      或者只改变节点个数，不改变特征的维数
      
      （节点个数会导致数据量变大，但似乎不影响层数。但是改变层数后模型行不收敛了）
      
      > 如果损失函数开始时几乎不变（或者说变化很小），然后逐渐开始收敛，这通常意味着以下几种情况之一：
      >
      > 1. **初始阶段数据稀疏或权重初始化不佳**：
      >    - 在训练初期，如果数据集很大，或者权重初始化不当（比如所有权重都初始化为零），那么模型可能需要一些时间来“热身”，即开始有效地调整其参数以适应数据。
      >
      > 2. **学习率设置**：
      >    - 如果使用的是一个固定的学习率（learning rate），且这个值设置得过小，那么在训练初期模型的更新可能非常缓慢，导致损失函数的变化不大。随着训练的进行，模型参数逐渐调整，损失函数开始下降并收敛。
      >
      > 3. **梯度消失或爆炸**：
      >    - 如果模型结构较深或者激活函数选择不当（如使用了sigmoid或tanh作为深层网络中的激活函数），可能会遇到梯度消失或梯度爆炸的问题。这会导致在训练初期权重更新非常缓慢甚至不动。解决这个问题的方法包括使用ReLU这样的激活函数，或者采用梯度裁剪等技术。
      >
      > 4. **局部最小值**：
      >    - 损失函数可能有多个局部最小值，在某些情况下，模型可能会陷入一个局部最小值，在这里损失函数的变化很小。随着进一步的训练，它可能会跳出这个局部最小值，并继续向全局最小值靠近。
      >
      > 5. **正则化效应**：
      >    - 如果使用了正则化技术（如L1或L2正则化），这些技术可以帮助避免过拟合，并可能导致损失函数在某些阶段变化较小。但是，随着训练的继续，模型会学习到更合适的参数，损失函数会继续下降。
      >
      > 6. **数据预处理**：
      >    - 数据预处理（如归一化、标准化）可以影响损失函数的变化速率。如果数据未正确预处理，可能会导致训练初期损失函数变化不大。
      >
      > 在任何情况下，重要的是要监控训练过程中的损失函数和其他相关指标，以确保模型能够有效地学习并且不会过拟合或欠拟合。如果有疑问，可以尝试调整学习率、使用不同的优化算法、改变模型架构或者增加/减少正则化强度等方法来改善训练过程。

==**计划**==

==先用效果好的小图导出模型，计算数值往后做==

## 241024

1. GAT 需要edge index ,但是edge index 不规则，不能npz

2. 构建graph pair 类

3. ```
   feature_1 = (feature_1 - mean_1) / std_1
   ```
   
   对每个特征进行标准化处理（也称为标准化或 z-score 标准化）是一种常用的数据预处理技术，目的是使每个特征具有零均值和单位方差。标准化处理可以改善数据的分布，从而提高机器学习模型的性能。以下是标准化处理的具体解释：
   
   #### **标准化处理公式**
   
   对于给定的一组数据 \($ \{x_1, x_2, \ldots, x_n\} $\)，其标准化处理公式为：
   
   \[$ z_i = \frac{x_i - \mu}{\sigma} $\]
   
   其中：
   - \( $z_i$ \) 是标准化后的数据；
   - \( $x_i$ \) 是原始数据；
   - \( $\mu$ \) 是原始数据的平均值（均值）；
   - \( $\sigma$ \) 是原始数据的标准差。
   
   #### **标准化的目的**
   
   1. **消除量纲影响**：不同特征可能有不同的量纲或尺度，标准化后可以消除这种差异，使特征在同一尺度上进行比较。
   2. **加速收敛**：在梯度下降等优化算法中，标准化后的数据可以使损失函数更快收敛。
   3. **提高模型性能**：对于一些机器学习算法（如支持向量机、线性回归等），标准化后的数据可以提高模型的性能。
   
   #### **总结**
   
   通过上述标准化处理，每个特征的平均值变为 0，标准差变为 1。这样可以使得特征在相同的尺度上进行比较，有助于提高模型的性能和收敛速度。如果你有进一步的问题或需要更具体的解释，请随时告诉我。
   
4. ```
   label_exp = torch.exp(-1 * label) # 无用
   ```

   您提到的将 `label` 转换为概率值的过程可以通过指数函数 \( $e^{-x}$ \) 来实现。这里我们来详细解释一下为什么这样做可以将正数转换为接近 0 的值，负数转换为接近 1 的值。

   #### **指数函数的性质**

   指数函数 \( $e^{-x}$ \) 有一些重要的性质：

   1. **当 \( x > 0 \)**：
      - \( $e^{-x}$ \) 的值会小于 1，并且随着 \( x \) 的增大，\( $e^{-x}$ \) 的值迅速趋近于 0。

   2. **当 \( x < 0 \)**：
      - \( $e^{-x}$ \) 的值会大于 1，并且随着 \( x \) 的减小，\( $e^{-x}$ \) 的值迅速趋近于无穷大。
      - 但是，如果我们关注的是 \( $e^{-|x|}$ \)，那么当 \( x < 0 \) 时，\( $e^{-x}$ \) 会变成 \( $e^{x}$ \)，此时 \($ e^{x}$ \) 的值会大于 1，并且随着 \( x \) 的减小，\( $e^{x}$ \) 的值会迅速趋近于 0。

   3. **当 \( x = 0 \)**：
      - \( $e^{-0} = e^{0} = 1$ \)。

   #### **举例说明**

   假设 `label` 是一个表示得分或距离的标量或向量，我们可以通过 \( e^{-x} \) 来将其转换为概率值：

   - **当 `label` 为正数**：
     - \( $e^{-\text{positive number}}$ \) 会接近 0，表示该得分或距离较高时，转换后的值接近 0。

   - **当 `label` 为负数**：
     - \( $e^{-\text{negative number}}$ \) 会接近 1，表示该得分或距离较低时，转换后的值接近 1。

   #### **修正为概率**

   如果我们希望将 `label` 转换为介于 0 和 1 之间的概率值，可以使用 Sigmoid 函数 \( $\sigma(x) = \frac{1}{1 + e^{-x}}$ \)。Sigmoid 函数确保所有的值都在 0 和 1 之间：

   #### **总结**

   通过 \( $e^{-x}$ \) 或 Sigmoid 函数 \( $\frac{1}{1 + e^{-x}}$ \)，可以将 `label` 转换为介于 0 和 1 之间的概率值。这种方法在很多场景中都非常有用，特别是在需要将得分或距离转换为概率值的情况下。如果你有具体的上下文或应用场景，请提供更多信息，以便我能给出更具体的解释和建议。

5. 粗心大意，输错矩阵，不是大小对不上GAT，是维度

6. ==如果用一个图对训练出模型，需要都训练一遍吗？（√）==还是后面的其实不用

   1. 不需要训练的数据怎么快速通过模型获得结果
   2. 他的模型没有考虑每个扰动图之间的关系？？
   3. 还是说图对能够把所有扰动图考虑进去

7. ==测试集对比一下吧（先不管了）==

8. ==某个图对的loss有尖刺==，什么原理，是否跟设备有关？？

   
## 241023※

1. 每个图对都分别计算相似度分数，计算N个？

与AI交流https://lxblog.com/qianwen/share?shareId=d1f6fc34-953b-4241-941d-a260f037fbaa

> 1. N个扰动图怎么凑图对：尽管存在N个扰动图，但每个扰动图都是独立地与基准图形成图对并通过相同的流程进行处理。
>
> 2. GAT中未扰动的节点也要学习表示？：GAT（图注意力网络）来学习节点表示时，即使某些节点本身没有改变，它们也会参与到其他节点特征的学习过程中，因为GAT是基于节点之间的相互作用来进行特征提取的。
>
>    此外，扰动图的设计目的是为了评估每个节点在网络中的贡献。通过对比扰动前后网络的变化情况，可以更好地理解每个节点的重要性。即使有些节点没有被直接扰动，它们在扰动图中的表现也会受到周围被扰动节点的影响，从而间接反映出它们在整个网络中的地位和功能。
>
>    综上所述，即使是那些在扰动图中没有直接发生改变的节点，它们的处理也不是多余的。
>
> 3. "图间"是？：相似度系数 $β_{mj}$ 和 $β_{in}$ 是用来衡量基准图 $G_c$ 中节点 $v_m$ 与扰动图 $G_o$ 中节点 $v^c_m$之间的相似度，以及基准图 $G_c$ 中节点 $v_i$与扰动图 $G_o$中节点 $v^i_c$之间的相似度。相似度系数是通过计算节点间表示向量的余弦相似度来获得的。
>
> 4. ”图间表示“的加权：图间表示是通过加权获得的原因在于它旨在捕捉两个图之间节点特征的关联性，并以此来反映节点在不同图中的相似性和重要性。这里所谓的“加权”，实际上是对节点在基准图和扰动图中表示的融合过程。
>
> 5. 加权的N：在相似度系数计算过程中，分母中的 N*N* 指的是图对中的另一个图中的节点数量。
>
> 6. 学习一个节点的图间表示，为什么$h^*_m$和$h^*_i$要计算2边：这种做法的目的在于评估节点在不同图结构下的表现一致性或差异性。
>
> 7. **全局读出：获得图的表示**
>
>    **基准图和扰动图的节点级图内（没有考虑彼此的相似度）和图间表示（考虑彼此的相似度）**
>
> 8. 交叉匹配：图对相似，则图对的图内表示和图间表示之间的差异很小
>
>    图间表示：假如图对相似，他们的图内表示相似，求的余弦相似度是这个点和其它节点的，**如果笼统的看作是节点在图中的表现的话肯定是相似，但具体而言呢？**相似度系数肯定笼统而言，是相近的，加权后的表示也是。
>
> 9. 越重要，相似度越低
>

2. pycharm更新导致代码不能运行？？？

   退回版本后可以了

```
from . import _csparsetools
ImportError: DLL load failed while importing _csparsetools: 动态链接库(DLL)初始化例程失败。
```

3. 写成函数似乎运行速度会变慢？？（通过计时，发现不会）
4. MGC的数据读取部分，以及模型函数的调用部分需要重构

## 241022

最近一直在忙汇报和实验课

关于扰动图，因为要考虑每个节点的重要性，所以每个节点都有一个扰动图

![image-20241022205231377](.\241012.assets\image-20241022205231377.png)

扰动确实是只扰动一个节点

> GAT  

​		那我GAT只需要对这一个节点做？（**对每个节点都做，虽然某些节点没有改变，但是会参与到其他节点的学习中去，扰动图设计的目的是为了评估每个节点在网络中的贡献，虽然某些节点没有直接被扰动，它们在扰动图中的表现也会受到周围被扰动节点的影响，从而间接反映出他们在整个网络中的地位和功能**）

​		**本章均采用均方误差作为损失函数**

​		采用重构损失？常见的重构损失包括均方误差（MSE）或交叉熵损失

> 加权求和（相似度系数）

1. 用相似度系数加权，是在和谁加权，N是谁？？总节点？？邻居？

​		GAT中已经考虑了邻居，图间表示是哪一步抽象？

​		~~不同图之间加权，对N个扰动图进行加权？~~ 

​		~~每个节点的扰动特性来源于N个扰动图中分别计算出的每个节点的表示，~~

​		~~可是，这样的话，大部分节点的表示都是相同的，每个扰动图只有一个节点只差~~

​		原图和扰动图分别图间表示

2. 相似度系数，是谁和谁相似，该节点在每个扰动图之间的表示

(图间表示没有搞懂，从图级表示入手倒推理解一下)

## 241017

文件调用 

内存不够了，内存映射，但是好像没有效果

只扰动了一个点，某个点都一个扰动图

图对是每个节点都有一对吗

两个图分别GAT 学习图表示？$G_c$有很多个图，怎么学习图表示

用传感器测量值作为节点特征这合理吗？？一段时间的温度数据是这个点的气候变化特征，对这个特征进行抽象，有什么意义吗

数据的特征，是该数据的抽象，表征，类似label，但不是准确的label



## 241016

重新下载数据集

整理报告的论文

## 241015 

写实验报告

## 241014

看了看课程汇报的素材。 但是没有定下题目。

## 241013

完成了无线投屏，确实略有延迟，但是可用

## 241012

配置了SSH的基本连接

pycharm需要专业版才能ssh连接

无线显示器方案
